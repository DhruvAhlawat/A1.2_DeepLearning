{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gensim #For word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import nltk\n",
    "import random\n",
    "from numpy.random import choice as randomchoice\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import sys\n",
    "# import torchtext\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import torchtext.vocab as vocab\n",
    "glove_model = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start_time = time.time();\n",
    "# val_file = sys.argv[2];\n",
    "train_file = 'data/train.json'\n",
    "val_file = 'data/dev.json'\n",
    "test_file = 'data/test.json'\n",
    "\n",
    "tokenize_func = nltk.tokenize.WordPunctTokenizer().tokenize\n",
    "punctuations = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'\n",
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError: #Classic way to get is_numeric\n",
    "        return False\n",
    "def tokenize(sentence, with_num = False):\n",
    "    old = tokenize_func(sentence.lower());\n",
    "    s = [];\n",
    "    for word in old:\n",
    "        running = [];\n",
    "        for character in word:\n",
    "            if(character in punctuations):\n",
    "                if(len(running) > 0):\n",
    "                    s.append(''.join(running));\n",
    "                    running = []; #emptying the running list.\n",
    "                s.append(character); #then adding the punctuation.\n",
    "            else:\n",
    "                running.append(character);\n",
    "        if(len(running) > 0):\n",
    "            s.append(''.join(running));\n",
    "        #this above code ensures that what we have is also split on punctuation\n",
    "    if(with_num):\n",
    "        return s; #If with_num is true, return the sentence as it is, without converting the numbers to <NUM>\n",
    "    for i in range(len(s)):\n",
    "        if(is_numeric(s[i])):\n",
    "            s[i] = '<NUM>'; #replaces numbers with <NUM>\n",
    "    return s;\n",
    "\n",
    "def tokenize_with_num(sentence): #just tokenizes normally. No replacement of numbers\n",
    "    s = tokenize_func(sentence.lower());\n",
    "    return s;\n",
    "\n",
    "def get_embedding_index(sentences, model):\n",
    "    return ([tokenize_and_get_embedding_index(sentence, model) for sentence in sentences]);\n",
    "\n",
    "\n",
    "def tokenize_and_get_embedding_index_as_list(sentence, vocab, with_num = False):\n",
    "    s = tokenize(sentence, with_num = with_num);\n",
    "    # FOr now testing with No UNK, Later will have to add UNK\n",
    "    tens = ([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n",
    "    return tens;\n",
    "\n",
    "def tokenize_and_get_embedding_index(sentence, vocab, with_num = False):\n",
    "    s = tokenize(sentence, with_num = with_num);\n",
    "    # FOr now testing with No UNK, Later will have to add UNK\n",
    "    tens = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n",
    "    return tens;\n",
    "    if(len(tens) == 0):\n",
    "        return torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) #using UNK in this case.\n",
    "    else:\n",
    "        return tens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(val_file) as f:\n",
    "    val_data = json.load(f)\n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_len = 120;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Postprocessing the code to remove the last '|' that is sometimes randomly present.\n",
    "def remove_last_extra(data):\n",
    "    for i in range(len(data)):\n",
    "        if(data[i]['linear_formula'][-1] == '|'):\n",
    "            data[i]['linear_formula'] = data[i]['linear_formula'][:-1];\n",
    "        \n",
    "    return data; #although not really needed.\n",
    "remove_last_extra(val_data);\n",
    "remove_last_extra(train_data);\n",
    "remove_last_extra(test_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glove_vectors():\n",
    "    def get_word_embedding(word, glove_vectors, dim):\n",
    "        if word in glove_vectors.key_to_index: #if the key is present we initialize it as glove embedding\n",
    "            return torch.tensor(glove_vectors[word])\n",
    "        else:\n",
    "            return torch.rand(dim)  # Initi\n",
    "    def __init__(self, sentences, glove_model, dim=200):\n",
    "        self.vocabulary = set(['<START>', '<END>', '<PAD>', '<UNK>', '<NUM>']);\n",
    "        for sentence in sentences:\n",
    "            for word in tokenize(sentence):\n",
    "                self.vocabulary.add(word); #creates the vocabulary.\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.vocabulary)};\n",
    "        self.index_to_word = {i: word for i, word in enumerate(self.vocabulary)};\n",
    "        self.wordvec = [0] * len(self.vocabulary); #initializing the encoder_wordvec list\n",
    "        rand_count = 0;\n",
    "        for i in range(len(self.vocabulary)):\n",
    "            self.wordvec[i] = glove_vectors.get_word_embedding(self.index_to_word[i], glove_model, dim);\n",
    "        self.wordvec = torch.stack(self.wordvec); #stacking the list of tensors to form a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vectors = glove_vectors([data['Problem'] for data in train_data], glove_model);\n",
    "decoder_vectors = glove_vectors([data['linear_formula'] for data in train_data], glove_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mysaved.pkl', 'wb') as f:\n",
    "    pickle.dump([encoder_vectors, decoder_vectors], f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_on_words(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index,bidirectional=True, dropout=0.0):\n",
    "        super(LSTM_on_words, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional).to(device);\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        # Embedding\n",
    "        out = self.embedding(x)\n",
    "        # Pack padded sequence\n",
    "        # lengths = x_lengths.detach().cpu().numpy();\n",
    "        out = pack_padded_sequence(out, x_lengths, batch_first=True, enforce_sorted=False).to(device);\n",
    "        out, (hidden, cell) = self.lstm(out)\n",
    "        # Unpack packed sequence\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out, (hidden, cell);\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layers = [];\n",
    "        self.ReLU = nn.ReLU(inplace=False)\n",
    "        for i in range(len(layer_sizes)):\n",
    "            if(i == 0):\n",
    "                self.layers.append(nn.Linear(input_size, layer_sizes[i]));\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]));\n",
    "            if(i != len(layer_sizes) - 1): #add Relu only if its not the last layer, since that is the output layer that we will softmax over.\n",
    "                self.layers.append(self.ReLU);\n",
    "        self.all_layers = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        out = self.all_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mathDataset(Dataset):\n",
    "    def __init__(self, data, global_max_len = global_max_len):\n",
    "        self.data = data;\n",
    "        # self.encoder_word_to_index = encoder_wordvec.word_to_index;\n",
    "        # self.vocab_index_to_word = vocab_index_to_word;\n",
    "        self.max_len = global_max_len;\n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.data[idx]['Problem'];\n",
    "        linear_formula = self.data[idx]['linear_formula']; #maybe the linear formula can go directly without getting emebdded as well.\n",
    "        problem = tokenize_and_get_embedding_index_as_list(problem, encoder_vectors.word_to_index);\n",
    "        problem.append(encoder_vectors.word_to_index['<END>']);\n",
    "        problem = torch.tensor(problem);\n",
    "        linear_formula = tokenize_and_get_embedding_index_as_list(linear_formula, decoder_vectors.word_to_index);\n",
    "        linear_formula.append(decoder_vectors.word_to_index['<END>']);\n",
    "        #we need this linear formula to be of a constant size.\n",
    "        padding_len = self.max_len - len(linear_formula)\n",
    "        linear_formula = linear_formula[:self.max_len];\n",
    "        if padding_len > 0:\n",
    "            linear_formula += [decoder_vectors.word_to_index['<PAD>']] * padding_len\n",
    "        linear_formula = torch.tensor(linear_formula)\n",
    "        return problem, linear_formula;\n",
    "\n",
    "def collate_fn(data):\n",
    "    # data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    problems, linear_formulas = zip(*data)\n",
    "    # problems = data; #zip(*data)\n",
    "    problems_lengths = [len(problem) for problem in problems]\n",
    "    # linear_formulas = pad_sequence\n",
    "    problems = pad_sequence(problems, batch_first=True, padding_value=encoder_vectors.word_to_index['<PAD>'])\n",
    "    linear_formulas = pad_sequence(linear_formulas, batch_first=True, padding_value=decoder_vectors.word_to_index['<PAD>'])\n",
    "    return problems, problems_lengths, linear_formulas;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mathDataset(train_data, global_max_len)\n",
    "batch_size = min(32, len(train_dataset));\n",
    "Train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn);\n",
    "Dev_loader = DataLoader(mathDataset(val_data, global_max_len), batch_size=batch_size, shuffle=True, collate_fn=collate_fn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so our encoder is simply LSTM_on_words. Now to make the decoder LSTM_on_words.\n",
    "teacher_forcing_probability = 0.6;\n",
    "class Decoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n",
    "        super(Decoder_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n",
    "        self.fc = FeedForward(hidden_size,[len(wordvectors)*2,len(wordvectors)]).to(device);\n",
    "        # self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n",
    "    def forward(self, batch_size,max_len,encoder_outputs, hidden, cell, teacher_forcing = None):\n",
    "        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n",
    "        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n",
    "        outputs = [];\n",
    "        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n",
    "        # print(\"teacher forcing:\", teacher_forcing.shape)\n",
    "        for i in range(max_len):\n",
    "            dec_out, (hidden, cell) = self.forward_step(dec_in,hidden, cell); #we get the value after one step of the LSTM.\n",
    "            outputs.append(dec_out);\n",
    "            # print(dec_out.shape);\n",
    "            if(teacher_forcing == None):\n",
    "                _, ind = dec_out.topk(1);\n",
    "                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "            else:\n",
    "                if(random.random() > teacher_forcing_probability):\n",
    "                    _, ind = dec_out.topk(1);\n",
    "                    dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "                else:\n",
    "                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n",
    "                    dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, (hidden, cell)\n",
    "    \n",
    "    def forward_step(self, inputs, hidden, cell):\n",
    "        outs = self.embedding(inputs);\n",
    "        outs, (h, c)  = self.lstm(outs, (hidden, cell));\n",
    "        outs = self.fc(outs); #\n",
    "        return outs, (h, c);\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, max_len):\n",
    "        super(Attention, self).__init__()\n",
    "        self.input_size = input_size;\n",
    "        self.max_len = max_len;\n",
    "        self.attn_calc = FeedForward(input_size, [input_size//2, 1]).to(device); # a simple 3 layer feed forward network. For fast training.\n",
    "        self.KeyMatrix = nn.Linear(input_size, input_size).to(device);\n",
    "        self.ValueMatrix = nn.Linear(input_size, input_size).to(device);\n",
    "        self.QueryMatrix = nn.Linear(input_size, input_size).to(device);\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs): \n",
    "        out = self.ValueMatrix(torch.tanh(self.QueryMatrix(hidden) + self.KeyMatrix(encoder_outputs)));\n",
    "        out = out.squeeze(2).unsqueeze(1);\n",
    "        weights = F.softmax(out, dim=-1);\n",
    "        context = torch.bmm(weights, encoder_outputs);\n",
    "        return context, weights;\n",
    "\n",
    "#now to use a decoder with attention we will need to do the following.\n",
    "class Decoder_LSTM_with_attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n",
    "        super(Decoder_LSTM_with_attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = Attention(self.hidden_size,global_max_len)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=False).to(device);\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n",
    "        self.fc = FeedForward(hidden_size,[len(wordvectors)*2,len(wordvectors)]).to(device);\n",
    "        # self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n",
    "    \n",
    "    def forward(self, batch_size,max_len,encoder_outputs, hidden, cell, teacher_forcing = None):\n",
    "        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n",
    "        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n",
    "        outputs = [];\n",
    "        attentions = [];\n",
    "        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n",
    "        # print(\"teacher forcing:\", teacher_forcing.shape)\n",
    "        for i in range(max_len):\n",
    "            dec_out, (hidden, cell), attention_weights = self.forward_step(dec_in,hidden, cell, encoder_outputs); #we get the value after one step of the LSTM.\n",
    "            outputs.append(dec_out);\n",
    "            attentions.append(attention_weights);\n",
    "            # print(dec_out.shape);\n",
    "            if(teacher_forcing == None):\n",
    "                _, ind = dec_out.topk(1);\n",
    "                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "            else:\n",
    "                if(random.random() > teacher_forcing_probability):\n",
    "                    _, ind = dec_out.topk(1);\n",
    "                    dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "                else:\n",
    "                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n",
    "                    dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, (hidden, cell), attentions\n",
    "    \n",
    "    def forward_step(self, inputs, hidden, cell, encoder_outputs):\n",
    "        outs = self.embedding(inputs);\n",
    "        query = hidden.permute(1,0,2);\n",
    "        context, attention_weights = self.attention(query, encoder_outputs);\n",
    "        inp = torch.cat((outs, context), dim=2); #concatenating them.\n",
    "        outs, (h, c)  = self.lstm(inp, (hidden, cell));\n",
    "        outs = self.fc(outs); #passing through feedforward.\n",
    "        return outs, (h, c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename);\n",
    "    model.load_state_dict(checkpoint['model_state_dict']);\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']);\n",
    "    epoch = checkpoint['epoch'];\n",
    "    loss = checkpoint['loss'];\n",
    "    return model, optimizer, epoch, loss;\n",
    "\n",
    "def store_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            }, filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self): #, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = LSTM_on_words(200, 200, 2, encoder_vectors.wordvec, encoder_vectors.word_to_index['<PAD>'], bidirectional=True).to(device)\n",
    "        #self.decoder = Decoder_LSTM(200, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n",
    "        self.decoder = Decoder_LSTM_with_attention(200, 400, 2, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device) \n",
    "\n",
    "model = seq2seq(); \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001); \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=decoder_vectors.word_to_index['<PAD>']);\n",
    "# criterion = nn.CrossEntropyLoss();\n",
    "num_epochs = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_curves = [];\n",
    "dev_loss_curves = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(dataloader, model, training = True, verbose=0):\n",
    "    running_sum = 0;\n",
    "    total_batches = len(dataloader);\n",
    "    done = 0;\n",
    "    final_loss = 0;\n",
    "    teacher_forcing = None;\n",
    "    if(training):\n",
    "        model.train();\n",
    "    else:\n",
    "        model.eval();\n",
    "    for i, (problems, problems_lengths, linear_formulas) in enumerate(dataloader):\n",
    "        problems = problems.to(device);\n",
    "        # problems_lengths = torch.tensor(problems_lengths).to(device);\n",
    "        linear_formulas = linear_formulas.to(device);\n",
    "        enc_out, (h_enc, c_enc) = model.encoder(problems, problems_lengths);\n",
    "        hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n",
    "        cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n",
    "        cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n",
    "    #    hidden = h_enc[-1].unsqueeze(0); cell = c_enc[-1].unsqueeze(0);\n",
    "        \n",
    "        outs, (h, c) = model.decoder(problems.shape[0],global_max_len,enc_out, hidden,cell, teacher_forcing = linear_formulas.unsqueeze(-1)) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n",
    "        #outs = decoder(batch_size,enc_out.shape[1], hidden, cell);\n",
    "        #the first type of decoder does not have any attention system, so what it will do is simply take the last hidden state of the encoder and decipher it further based on that.\n",
    "        loss = criterion(outs.view(-1, outs.shape[-1]), linear_formulas.view(-1));\n",
    "        done += 1;\n",
    "        running_sum += loss.item();\n",
    "        if(training):\n",
    "            optimizer.zero_grad();\n",
    "            loss.backward();\n",
    "            optimizer.step(); \n",
    "        if(verbose > 0):  \n",
    "            print(\"batch:\", i,\" out of\", total_batches,\" running loss:\",running_sum/done, end = \"                                   \\r\");\n",
    "    final_loss = running_sum/total_batches; #this is the average loss.\n",
    "    model.train(); #sets it back into training mode.\n",
    "    return final_loss; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [];\n",
    "dev_loss = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m running_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[0;32m      5\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[1;32m----> 6\u001b[0m cur_epoch_loss \u001b[38;5;241m=\u001b[39m run_model(Train_loader, model, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m);\n\u001b[0;32m      7\u001b[0m cur_dev_loss \u001b[38;5;241m=\u001b[39m run_model(Dev_loader, model, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m); \u001b[38;5;66;03m#not training on the dev set.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(cur_epoch_loss);\n",
      "Cell \u001b[1;32mIn[36], line 22\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(dataloader, model, training, verbose)\u001b[0m\n\u001b[0;32m     19\u001b[0m     cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cell[\u001b[38;5;241m0\u001b[39m], cell[\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m); \u001b[38;5;66;03m#reverse and forward direction.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#    hidden = h_enc[-1].unsqueeze(0); cell = c_enc[-1].unsqueeze(0);\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     outs, (h, c) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(problems\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],global_max_len,enc_out, hidden,cell, teacher_forcing \u001b[38;5;241m=\u001b[39m linear_formulas\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#, teacher_forcing = linear_formulas.unsqueeze(-1));\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#outs = decoder(batch_size,enc_out.shape[1], hidden, cell);\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#the first type of decoder does not have any attention system, so what it will do is simply take the last hidden state of the encoder and decipher it further based on that.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), linear_formulas\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m));\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 80\u001b[0m, in \u001b[0;36mDecoder_LSTM_with_attention.forward\u001b[1;34m(self, batch_size, max_len, encoder_outputs, hidden, cell, teacher_forcing)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# print(\"teacher forcing:\", teacher_forcing.shape)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[1;32m---> 80\u001b[0m     dec_out, (hidden, cell), attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_step(dec_in,hidden, cell, encoder_outputs); \u001b[38;5;66;03m#we get the value after one step of the LSTM.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(dec_out);\n\u001b[0;32m     82\u001b[0m     attentions\u001b[38;5;241m.\u001b[39mappend(attention_weights);\n",
      "Cell \u001b[1;32mIn[29], line 100\u001b[0m, in \u001b[0;36mDecoder_LSTM_with_attention.forward_step\u001b[1;34m(self, inputs, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[0;32m     98\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(inputs);\n\u001b[0;32m     99\u001b[0m query \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m);\n\u001b[1;32m--> 100\u001b[0m context, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(query, encoder_outputs);\n\u001b[0;32m    101\u001b[0m inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((outs, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m); \u001b[38;5;66;03m#concatenating them.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m outs, (h, c)  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(inp, (hidden, cell));\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 57\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m);\n\u001b[0;32m     56\u001b[0m weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m);\n\u001b[1;32m---> 57\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(weights, encoder_outputs);\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context, weights\n",
      "\u001b[1;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "total_batches = len(Train_loader);\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time();\n",
    "    running_sum = 0;\n",
    "    done = 0;\n",
    "    cur_epoch_loss = run_model(Train_loader, model, training = True, verbose=1);\n",
    "    cur_dev_loss = run_model(Dev_loader, model, training = False, verbose=1); #not training on the dev set.\n",
    "    train_loss.append(cur_epoch_loss);\n",
    "    dev_loss.append(cur_dev_loss);\n",
    "    print(\"Epoch: \", epoch, \"avg loss:\", cur_epoch_loss, \"dev loss:\", cur_dev_loss, \"Time taken:\", time.time() - epoch_start);\n",
    "    store_checkpoint(model, optimizer, epoch, cur_epoch_loss, 'data/checkpoint' + str(epoch) + '.pth');\n",
    "    #now we will run it on the dev set to get loss curves.\n",
    "    with open('math_loss_curves.pkl', 'wb') as f:\n",
    "        pickle.dump([train_loss, dev_loss], f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will save the lists that we have prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4oAAAI6CAYAAABhMLY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSKElEQVR4nO3dd5hU9d3//9e0ne27LLC7IIssRekdAQuCghrLV6LRYMUSY2wJ8WdiokluTZFbE413YokaSxIbsZtYAQXESgcp0ou0pW5lZ3Zmzu+Ps7szh7r9nJl5Pq5rL9nPDLtv/G7uL08/53yOyzAMQwAAAAAA1HLbPQAAAAAAwFkIRQAAAACABaEIAAAAALAgFAEAAAAAFl67BwAAAACAxgqHw6qpqbF7jLjl8/nk8XiO+DqhCAAAACBuGIahHTt2aP/+/XaPEvdyc3NVWFgol8t1yGuEIgAAAIC4UReJ+fn5Sk9PP2zk4OgMw1BVVZVKSkokSZ06dTrkPYQiAAAAgLgQDofrI7F9+/Z2jxPX0tLSJEklJSXKz88/5DJUDrMBAAAAEBfq7klMT0+3eZLEUPfv8XD3ehKKAAAAAOIKl5u2jKP9eyQUAQAAAAAWhCIAAAAAwIJQBAAAAIA4NHbsWE2ZMqVVvjanngIAAABAKzrWPZWTJ0/Wc8891+iv+/rrr8vn8zVxqqMjFAEAAACgFW3fvr3+19OmTdNvfvMbffPNN/VrdY+qqFNTU9OgAMzLy2u5IQ/CpacAAAAA4pZhGKoKhtr8wzCMBs9YWFhY/5GTkyOXy1X/eXV1tXJzc/Xvf/9bY8eOVWpqqp5//nnt2bNHl112mbp06aL09HQNGDBAL730kuXrHnzpabdu3XTffffpuuuuU1ZWlrp27aonn3yySf9e2VEEAAAAELcO1ITV9zcftPn3XfHbs5We0nI5deedd+rBBx/Us88+K7/fr+rqag0bNkx33nmnsrOz9c477+iqq65S9+7dNXLkyCN+nQcffFC/+93vdNddd+nVV1/VTTfdpDFjxqh3796NmodQBAAAAACbTZkyRRdddJFl7Y477qj/9W233ab3339fr7zyylFD8dxzz9XNN98syYzPP//5z5o1axahCAAAACB5pPk8WvHbs235vi1p+PDhls/D4bD+93//V9OmTdPWrVsVCAQUCASUkZFx1K8zcODA+l/XXeJaUlLS6HkIRQAAAABxy+VytegloHY5OAAffPBB/fnPf9bDDz+sAQMGKCMjQ1OmTFEwGDzq1zn4EByXy6VIJNLoeeL/3ygAAAAAJJhPPvlEF154oa688kpJUiQS0Zo1a9SnT582+f6cegoAAAAADtOzZ09Nnz5dn332mVauXKkbb7xRO3bsaLPvTygCAAAAgMP8+te/1tChQ3X22Wdr7NixKiws1MSJE9vs+7uMxjwABAAAAABsUl1drQ0bNqi4uFipqal2jxP3jvbvkx1FAAAAAIAFoQgAAAAAsCAUAQAAAAAWhCIAAAAAwIJQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAAK3smmuukcvlksvlks/nU0FBgSZMmKBnnnlGkUjE7vEOQSgCAAAAQBs455xztH37dm3cuFHvvfeexo0bp5/85Cc6//zzFQqF7B7PglAEAAAAgDbg9/tVWFio4447TkOHDtVdd92lt956S++9956ee+45SVJpaal++MMfKj8/X9nZ2TrjjDO0ZMkSSdI333wjl8ulVatWWb7uQw89pG7duskwjBablVAEAAAAEL8MQwpWtv1HC0XZGWecoUGDBun111+XYRg677zztGPHDr377rtasGCBhg4dqjPPPFN79+7ViSeeqGHDhumFF16wfI0XX3xRl19+uVwuV4vMJEneFvtKAAAAANDWaqqk+zq3/fe9a5uUktEiX6p3795aunSpPv74Yy1btkwlJSXy+/2SpD/96U9688039eqrr+qHP/yhrrjiCj3yyCP63e9+J0lavXq1FixYoH/+858tMksddhQBAAAAwEaGYcjlcmnBggWqqKhQ+/btlZmZWf+xYcMGrVu3TpI0adIkbdq0SV988YUk6YUXXtDgwYPVt2/fFp2JHUUAAAAA8cuXbu7u2fF9W8jKlStVXFysSCSiTp06adasWYe8Jzc3V5LUqVMnjRs3Ti+++KJGjRqll156STfeeGOLzVKHUAQAAAAQv1yuFrsE1A4fffSRli1bpp/+9Kfq0qWLduzYIa/Xq27duh3x91xxxRW68847ddlll2ndunWaNGlSi8/FpacAAAAA0AYCgYB27NihrVu3auHChbrvvvt04YUX6vzzz9fVV1+t8ePHa/To0Zo4caI++OADbdy4UZ999pl+9atfaf78+fVf56KLLlJZWZluuukmjRs3Tscdd1yLz8qOIgAAAAC0gffff1+dOnWS1+tVu3btNGjQIP3lL3/R5MmT5Xabe3jvvvuu7r77bl133XXatWuXCgsLNWbMGBUUFNR/nezsbF1wwQV65ZVX9Mwzz7TKrC6jJR+2AQAAAACtpLq6Whs2bFBxcbFSU1PtHifuHe3fJ5eeAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAIC4wnmcLeNo/x4JRQAAAABxwefzSZKqqqpsniQx1P17rPv3GovnKAIAAACICx6PR7m5uSopKZEkpaeny+Vy2TxV/DEMQ1VVVSopKVFubq48Hs8h7+E5igAAAADihmEY2rFjh/bv32/3KHEvNzdXhYWFh41tQhEAAABA3AmHw6qpqbF7jLjl8/kOu5NYh1AEAAAAAFhwmA0AAAAAwIJQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFh42/obRiIRbdu2TVlZWXK5XG397QEAAAA4hGEYKi8vV+fOneV2s4flJG0eitu2bVNRUVFbf1sAAAAADrVlyxZ16dLF7jEQo81DMSsrS5L5w5Cdnd3W3x4AAACAQ5SVlamoqKi+EeAcbR6KdZebZmdnE4oAAAAAuCXNgbgQGAAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFgQigAAAAAAi+QOxXDI7gkAAAAAwHGSNxSDldJfBkvv3Snt3WD3NAAAAADgGMkbiivelkq3SF/+TfrrUGnaldLmLyTDsHsyAAAAALBV8obioEnSla9LPc6UjIi08j/SM2dLfz9TWvaqFK6xe0IAAAAAsIXLMNp2C62srEw5OTkqLS1VdnZ2W37rI9u5QvriMWnpv6VwwFzL7iKN/KE0dLKUlmvreAAAAEAicmQbQBKhaFWxS5r/tPTVU1LVbnPNlyENvUoa+SMpr9je+QAAAIAE4ug2SHKE4uHUVEvL/i19/pi0a2Xtokvqc7406hap6yjJ5bJ1RAAAACDexUUbJClC8WgMQ1r3kfT5o9K6mdH1zkOl0bdIfS+UPD775gMAAADiWFy1QZIhFBuqZKV5H+OSaTH3MR4njbyR+xgBAACAJojbNkgChGJjHfU+xhulvO72zgcAAADEibhvgwRGKDZVTbW07BXzstTY+xh7nyeNvpX7GAEAAIBjSJg2SECEYnPV3cf4xWPS2hnRde5jBAAAAI4q4doggRCKLYn7GAEAAIAGS+g2iHOEYmuo2CXNf0aa95RUuctc82VIQ66URv2I+xgBAAAAJUkbxClCsTUd9T7GW6Suo7mPEQAAAEkrqdogzhCKbcEwpPUfm8FouY9xiHnwDfcxAgAAIAklZRvECUKxrXEfIwAAACCJNnAyQtEu3McIAACAJEcbOBehaLeaaunrV83LUktW1C5yHyMAAAASH23gXISiU3AfIwAAAJIMbeBchKITlayqvY/xZet9jCf9UBo2WUprZ+98AAAAQAugDZyLUHQy7mMEAABAAqMNnItQjAfcxwgAAIAERBs4F6EYT+rvY3xMWjs9ut5psHkfY7+J3McIAACAuEEbOBehGK8Odx9jVmfzeYzcxwgAAIA4QBs4F6EY7yp3m/cxfvXkQfcxXiGN/JHUvoe98wEAAABHQBs4l7sxb77nnnvkcrksH4WFha01Gxoio4N0+s+lKV9LFz4q5feTairNcPzrMOmly6WNn5qXrQIAAABAA3gb+xv69eunGTOiz/nzeDwtOhCayJdqnoY6+App/aza5zFOl755x/zgPkYAAAAADdToUPR6vewiOpnLJfUYZ37s+iZ6H+P2xdLrP5Cm/0Ya+UNp2DXcxwgAAADgsBp16akkrVmzRp07d1ZxcbEmTZqk9evXH/X9gUBAZWVllg+0kY4nShf8n/TT5dK4u6WMjlL5NmnGPdJDfaV3fybtWWf3lAAAAAAcplGH2bz33nuqqqrSCSecoJ07d+r3v/+9Vq1apeXLl6t9+/aH/T333HOP7r333kPWuWHVBvXPY3xMKlleu+iSTjzXfB7j8SfzPEYAAAC0GQ6zca5mnXpaWVmpHj166Oc//7luv/32w74nEAgoEAjUf15WVqaioiJ+GOxkGNb7GOt0GmwGY7/vch8jAAAAWh2h6FyNvkcxVkZGhgYMGKA1a9Yc8T1+v19+v7853wYt7aj3Md4gTf8f6aQbpKGTpYzD7xQDAAAASFyNvkcxViAQ0MqVK9WpU6eWmgdt7ZD7GPPN+xhn3is91Ed6/UZpyzwerwEAAAAkkUZdenrHHXfoggsuUNeuXVVSUqLf//73mj17tpYtW6bjjz++QV+D7WWHCwWkZa9IXz4h7VgaXS8cKI34gTTge1JKhn3zAQAAIGHQBs7VqB3Fb7/9VpdddplOPPFEXXTRRUpJSdEXX3zR4EhEHPD6zecx3jhH+sFMadDlksdvRuN/fiw92Ed67xfSrtV2TwoAAACglTTrMJum4L8axKGqvdKi56X5T0v7NkbXi8eYu4wnnsvhNwAAAGg02sC5CEU0XCQirftImvd3afX7kmp/dLI6ScOuMQ+/yeZ+VQAAADQMbeBchCKaZt8macFz0sJ/SlW7zTWXR+pzvrnL2O00nskIAACAo6INnItQRPOEAtLK/5i7jJs/j653OEEafr00aJKUlmvbeAAAAHAu2sC5CEW0nB1fm/cxLpkm1VSaa750acAl5i5jp4H2zgcAAABHoQ2ci1BEy6suk5ZOM3cZd62Krnc5yQzGvhdKvlT75gMAAIAj0AbORSii9RiGtOkzMxhXvi1FQuZ6entpyFXS8Guldt1sHREAAAD2oQ2ci1BE2yjfaR58s+BZqWxr7aJL6nWWNOJ6qed4ye2xdUQAAAC0LdrAuQhFtK1wyHy0xry/S+s/jq7ndpWGX2fuNGZ0sG8+AAAAtBnawLkIRdhnzzpp/jPSon9J1aXmmidF6vdd817GLiN4xAYAAEACow2ci1CE/YJV0vLXpa+ekrYvjq4XDjCDccAlUkqGbeMBAACgddAGzkUowlm2LpDmPS19/ZoUqjbX/NnSoMvMexk7nmjvfAAAAGgxtIFzEYpwpqq90uIXzEtT966Prnc7zdxl7H2e5PHZNx8AAACajTZwLkIRzhaJmIfezHtaWv2eZETM9cxCadg10rDJUnZnW0cEAABA09AGzkUoIn7s3yIteE5a+A+pcpe55vJIvc81dxmLT+fwGwAAgDhCGzgXoYj4EwpKK982dxk3fxZdb9/LvI9x0GVSWq5t4wEAAKBhaAPnIhQR33aukOY/LS15WQpWmGveNGngJdLw66XOg20dDwAAAEdGGzgXoYjEECiXlk4zdxlLVkTXu4wwL0vtO1Hypdo2HgAAAA5FGzgXoYjEYhjS5s+leX+XVrwtRWrM9bQ8aehV0rBrpbxie2cEAACAJNrAyQhFJK7yndKif0rzn5PKvq1ddEk9x5u7jL0mSG6PnRMCAAAkNdrAuQhFJL5wSFrzgXlZ6rqZ0fWcrtLwa6UhV0mZHe2bDwAAIEnRBs5FKCK57FknzX9GWvS8VL3fXPOkmPcwjviBVHQSj9gAAABoI7SBcxGKSE41B6SvXzfvZdy2MLpe0N98xMaASyV/pn3zAQAAJAHawLkIRWDrAmneM9LXr0qhanMtJUsafJn5iI383vbOBwAAkKBoA+ciFIE6VXulJS+Zu4x710fXu51m7jL2Pl/y+OybDwAAIMHQBs5FKAIHi0SkDbPMw2++eVcyIuZ6ZoE0dLI0aJLUvoetIwIAACQC2sC5CEXgaEq/lRY8Jy34h1RZEl3P6y71Ost8xMbxp0q+VNtGBAAAiFe0gXMRikBDhILSqv9KC/8pbZwrRWqir/nSpeIxZjT2OkvK7WrfnAAAAHGENnAuQhForEC5tH62tOZDac10qXyb9fWOfaLR2HUU9zUCAAAcAW3gXIQi0ByGIe1cHo3GLV9KRjj6uj9b6j42eplqVqFtowIAADgNbeBchCLQkg7sk9Z9ZEbjmulS1W7r64UDpRPONsPxuGGS22PPnAAAAA5AGzgXoQi0lkhE2r6oNho/lLYulBTzP7e0dlLP8WY09jhTymhv26gAAAB2oA2ci1AE2krFLmntDDMa182UqktjXnRJXYZHL1EtHCS53baNCgAA0BZoA+ciFAE7hEPSt/Oi9zbuXGZ9PbNA6jnBjMYe46TUHHvmBAAAaEW0gXMRioATlG6N7jaunyUFK6Kvub1S0ajoSar5fSSXy7ZRAQAAWgpt4FyEIuA0oYC0+fPovY27V1tfz+4Sjcbup0spGfbMCQAA0Ey0gXMRioDT7d0Q3W3cMEcKVUdf86RI3U6tvbfxLKl9D/vmBAAAaCTawLkIRSCe1ByQNs6VVn8grflA2r/Z+npe9+iBOMefKvlS7ZkTAACgAWgD5yIUgXhlGNLuNbUH4nwobfpMitREX/elS8Vjopep5na1b1YAAIDDoA2ci1AEEkWgXFo/O3qSavk26+sd+0SjsesoyeOzZ04AAIBatIFzEYpAIjIMaedy8/LUNdOlLV9KRiT6uj9b6j42eplqVqFtowIAgORFGzgXoQgkgwP7pHUf1Z6kOl2q2m19vXCgdMLZZjgeN0xye+yZEwAAJBXawLkIRSDZRCLS9kXRx29sXSgp5v8MpLWTeo43o7HHmVJGe9tGBQAAiY02cC5CEUh2Fbuij99YN1OqLo150SV1GR69RLVwkOR22zYqAABILLSBcxGKAKLCIenbedEDcXYus76eWSD1nGBGY49xUmqOPXMCAICEQBs4F6EI4MhKt0pra+9rXPexVFMZfc3tlYpGRU9Sze8juVz2zQoAAOIObeBchCKAhgkFpM2fR+9t3L3a+nq7btLJt0lDrpK8fltGBAAA8YU2cC5CEUDT7N0QvbdxwxwpVG2uZ3WWTvmJNGyy5Euzd0YAAOBotIFzEYoAmi9YKS16Xpr7sFS+zVzLyDd3GIdfJ/kzbR0PAAA4E23gXIQigJYTCkSDsXSzuZaWJ518qzTiBimV/80DAIAo2sC5CEUALS9cIy15WfrkQWnfBnMtNVcadZM08kbzWY0AACDp0QbORSgCaD3hkPT1a9KcP0p71phr/mzppBukUbdIGe3tnQ8AANiKNnAuQhFA64uEpRVvSnP+JJWsMNd8GdKI6837GDPzbR0PAADYgzZwLkIRQNuJRKRv3pFmPyDtWGquedOkYdeYJ6Vmd7J1PAAA0LZoA+ciFAG0PcOQVn8gzXlA2rrAXPP4paFXSadMkXKLbB0PAAC0DdrAuQhFAPYxDGndR+Y9jJs/N9fcPmnwZdKpt0t5xfbOBwAAWhVt4FyEIgD7GYa0ca40+35p4yfmmssjDfy+dNr/J3Xoae98AACgVdAGzuW2ewAAkMslFZ8mXfNf6boPpB5nSkZYWvKi9OgI6dXrpZKVdk8JAACQNAhFAM7SdZR01evSDz6STjhHMiLS169Kj42W/n21tGOZ3RMCAAAkPEIRgDN1GSZdPk26cY7U5wJJhrTiLelvp0ovXSZtXWj3hAAAAAmLUATgbJ0GSd9/Xrrpc6n/xZJc0jfvSk+Nk56/WNr8pd0TAgAAJBxCEUB8KOgrfe8Z6ZavpIGTzMNu1s6QnjlL+sf/Mw/DAQAAQIvg1FMA8WnveumTh6QlL0mRkLnW9WTp9J9L3ceaB+QAAABHow2ci1AEEN/2b5bmPiwt+pcUDpprXUZIY34u9ZpAMAIA4GC0gXMRigASQ9k26dP/kxY8J4WqzbVOg6UxP5NOPFdyc6U9AABOQxs4F6EIILGU75Q+/6s072mppspcK+gvjblD6nMhwQgAgIPQBs5FKAJITJV7pC8elb58UgqWm2sdTjSDsd9Fksdr73wAAIA2cDBCEUBiq9orffmE9OXjUnWpuZbXXTrtDmngpZLHZ+98AAAkMdrAuQhFAMmhulT66inp80elA3vNtdyu0qm3S4Mvl7x+e+cDACAJ0QbORSgCSC6BCmn+09Jnf5Uqd5lr2cdJp0yRhl4t+VJtHQ8AgGRCGzgXoQggOQWrpIX/ME9KLd9urmUWSqf8WBp2rZSSbu98AAAkAdrAuQhFAMmtptp8BuPch6Wyb8219A7SybdKI34g+bNsHQ8AgERGGzgXoQgAkhQKSktekuY+JO3baK6ltZNG3SKN/KGUmmPreAAAJCLawLkIRQCIFQ5Jy16RPvmTtGetuebPkUbeKI26SUrPs3c+AAASCG3gXM168vTUqVPlcrk0ZcqUFhoHAGzm8UqDL5Nu+Uq6+GmpY28pUCrNeUB6eIA04x6pcrfdUwIAALSqJofivHnz9OSTT2rgwIEtOQ8AOIPbIw34nnTT59Kl/5QKBkjBCmnun81g/OBuqXyH3VMCAAC0iiaFYkVFha644go99dRTateu3VHfGwgEVFZWZvkAgLjhdkt9L5R+9Ik06SWp8xCppkr6/BHp4YHSuz+TSrfaPSUAAECLalIo3nLLLTrvvPM0fvz4Y7536tSpysnJqf8oKipqyrcEAHu5XFLvc6UbPpaueE0qGimFA9JXT0p/GSz9Z4q0b5PdUwIAALSIRofiyy+/rIULF2rq1KkNev8vf/lLlZaW1n9s2bKl0UMCgGO4XFKv8dJ1H0hXvy11O00KB6UFz0p/HSq9eYu0Z53dUwIAADSLtzFv3rJli37yk5/oww8/VGpqaoN+j9/vl9/vb9JwAOBYLpfU/XTzY9Nn0uwHpPUfS4ufl5a8KA24RDrtDqnjCXZPCgAA0GiNejzGm2++qe9+97vyeDz1a+FwWC6XS263W4FAwPLa4XAELoCEtWWeNOeP0poPahdcUr+J0ri7pQ697JwMAABHog2cq1GhWF5erk2brPfgXHvtterdu7fuvPNO9e/f/5hfgx8GAAlv2yJpzp+kVf81P/ekSKdMkU67XfKl2ToaAABOQhs4V6MuPc3KyjokBjMyMtS+ffsGRSIAJIXOQ6RJL0g7vpZm3iut+dB8DuOyV6Tz/iT1PPZBYAAAAHZq8nMUAQDHUNhfuvzf0qX/krI6S/s2SM9fLL1yjVS23e7pAAAAjqhRl562BLaXASSlQLn08VTpy8clIyKlZEln/loa8QPJffR7uwEASFS0gXOxowgAbcGfJZ1zn/TDWdJxw6VgufTez6WnzpC2LrR7OgAAAAtCEQDaUqdB0vXTpfP/LKXmSNsXm7H4zh1Sdand0wEAAEgiFAGg7bnd0vDrpFvnSwO/L8mQ5j0lPTJCWvaq1LZ3BAAAAByCUAQAu2TmSxc9KV39ttS+p1SxU3rteulfE6U96+yeDgAAJDFCEQDs1v106abPpHF3Sx6/tH6W9Nhoadb/SjXVdk8HAACSEKEIAE7g9Uun/1y6+XOpxxlSOCDNmio9frK07mO7pwMAAEmGUAQAJ2nfQ7rydel7z0qZhdLedealqK9eL5XvtHs6AACQJAhFAHAal0vqf5F061fSSTdKLrf09avmYTdfPSVFwnZPCAAAEhyhCABOlZojnfuAdMNHUuchUqBUevcO6e/jpW2L7Z4OAAAkMEIRAJyu8xDpBzOlc/8k+bOlbQulp8ZJ790pVZfZPR0AAEhAhCIAxAO3RzrpBunWeVL/iyUjIn35N/Ny1OVv8OxFAADQoghFAIgnWYXS956RrnpDyusuVeyQXrlGeuF70t71dk8HAAASBKEIAPGoxxnSTZ9Lp/9C8qRIa2eYz16c/UcpFLB7OgAAEOcIRQCIV75UadwvzWAsPl0KVUsf/156/BRpwxy7pwMAAHGMUASAeNehp3T1W9JFf5cy8qU9a6R/XCC9fqNUscvu6QAAQBwiFAEgEbhc0sBLzMNuRvxAkkta+rL0yDBp/rNSJGL3hAAAII4QigCQSNJypfMeNB+nUThQqi6V/jtFeuYsaccyu6cDAABxglAEgETUZZh0w8fSOf8rpWRJ386Tnjhd+uBuKVBu93QAAMDhCEUASFQerzTqJunWr6S+EyUjLH3+iPToSGnlf3j2IgAAOCJCEQASXXZn6dJ/SFe8KuUeL5VtlaZdKb34fWnfRrunAwAADkQoAkCy6DVBuuVLaczPJLdPWvOB9Ogo6ZOHpFDQ7ukAAICDEIoAkEx8adIZv5Ju+kzqdpoUOiDNvFd64jRp46d2TwcAAByCUASAZNTxBGnyf6TvPiGld5B2rZKeO1d682apcrfd0wEAAJsRigCQrFwuadAk89mLw64x1xa/ID0yXFr4T569CABAEiMUASDZpedJF/yfdP10qaC/dGCf9PZt0rPfkXYut3s6AABgA0IRAGAqOkn64WzprD9IvgxpyxfSE2OkD38tBSvtng4AALQhQhEAEOXxSiffaj57sc8FUiQkffYX89mLq961ezoAANBGCEUAwKFyukjff166bJqU01Uq3SK9fJn00uXS/i12TwcAAFoZoQgAOLITzzGfvXjqTyW3V/rmHenRk6RP/08K19g9HQAAaCWEIgDg6FLSpfH3SD+aK3U9Waqpkqb/xrx/cfMXdk8HAABaAaEIAGiY/D7Ste9KFz4mpeVJJSukZ842T0it2mv3dAAAoAURigCAhnO5pCFXSLctkIZcZa4t/Kf57MVFL0iGYe98AACgRRCKAIDGS8+TLnxEuu4DKb+vVLVHeutm6bnzpJJVdk8HAACaiVAEADRd11HSjXOkCb+VfOnSpk+lv50izbhXClbZPR0AAGgiQhEA0Dwen3TKT8zTUU8813z24tyHpMdGSqs/sHs6AADQBIQiAKBl5HaVLntJmvSilN1F2r9ZevFSadqVUulWu6cDAACNQCgCAFpW7/PM3cWTfyy5PNLK/5jPXvz8USkcsns6AADQAIQiAKDl+TOls35n3r9YNFIKVkgf3CU9OVbaMs/u6QAAwDEQigCA1lPYX7r2fen//VVKayftXCY9PUH6zxTpwD67pwMAAEdAKAIAWpfbLQ29Wrp1vjT4CkmGtOBZ6S9DpI/vkyp32z0hAAA4CKEIAGgbGR2kiY9J17wjdext7ijOvl/6c3/pnTukvRvsnhAAANRyGYZhtOU3LCsrU05OjkpLS5Wdnd2W3xoA4BSRsLTybWnuw9L2xeaayy31nWg+aqPzYPtmAwC0GdrAuQhFAIB9DEPaMEf69P+kdTOj693HmsHYfZzkctk2HgCgddEGzkUoAgCcYccyMxi/fl0ywuZa4UAzGPtOlDxeW8cDALQ82sC5CEUAgLPs2yR98Zi08J9STZW5lttVGn2bNORKKSXd3vkAAC2GNnAuQhEA4ExVe6V5f5e+/JtUtcdcS8uTRt4ojbhBymhv73wAgGajDZyLUAQAOFuwSlr8gvTZX6X9m8w1b5o09Cpp9C1Su262jgcAaDrawLkIRQBAfAiHpJVvmfcxbl9irrncUr/vmvcxdhpk73wAgEajDZyLUAQAxBfDkDbMrj0p9aPoevdxtSeljuWkVACIE7SBcxGKAID4tX2J9OlfpOVvRE9K7TTIDMY+F3JSKgA4HG3gXIQiACD+7dsofV57UmrogLnWrps0+lZp8BWclAoADkUbOBehCABIHJV7pHlPSV8+IR3Ya66lt5dOulE66QYpPc/e+QAAFrSBcxGKAIDEU39S6l+k/ZvNNV+6NPRqadTNUrvj7Z0PACCJNnAyQhEAkLjCIWnFm+bBNzuWmmsuj9T/IunkH0udBto6HgAkO9rAuQhFAEDiMwxp/SwzGNd/HF3vcYZ0yhSpeAwnpQKADWgD5yIUAQDJZdti85LU5W9IRsRc6zTYPCm174WS22PndACQVGgD5yIUAQDJad9G6bNHpEXPW09KPfk286RUX5qd0wFAUqANnItQBAAkt8rd0ldPSV89IR3YZ66ld5BG/kgacT0npQJAK6INnItQBABAkoKV0qIXpM//GnNSaoZ5Uurom6XcrvbOBwAJiDZwLkIRAIBY9SelPiztWGauuTxS/4vN+xgL+9s5HQAkFNrAuQhFAAAOxzDME1LnPixtmB1d7zneDMZup3FSKgA0E23gXIQiAADHsm2R9OlfzJ3GupNSOw81g7HPBZyUCgBNRBs4F6EIAEBD7V0vff5o7Ump1eZau+Lak1Iv56RUAGgk2sC5CEUAABqrcrf01ZPmR91JqRkdpZE3SsM5KRUAGoo2cC5CEQCApgpWSgv/JX3+iFS6xVzzZUjDJkujbpZyi+ydDwAcjjZwLkIRAIDmCtdIy9+UPv0/aWftSalur9T/e9IpP5YK+tk6HgA4FW3gXIQiAAAtxTCkdTPNYNwwJ7rec4J06hTp+FM4KRUAYtAGzkUoAgDQGrYuNINx5dvRk1KPG2aelNr7fE5KBQDRBk5GKAIA0Jr2rDNPSl38QvSk1Lwe5kmpgy6TfKn2zgcANqINnItQBACgLVTsip6UWr3fXMvIN09KHXG9lNbO1vEAwA60gXMRigAAtKVAhbToX+YuY91JqSmZ0rBrpFE3STldbB0PANoSbeBchCIAAHYI10hfv27ex1iy3Fxze6UBl0gjfyR1HmzreADQFmgD5yIUAQCwk2FIa2dKnz4sbfwkun7cMGn49VL/iyRfmm3jAUBrog2cy92YNz/++OMaOHCgsrOzlZ2drdGjR+u9995rrdkAAEh8LpfUa7x0zX+lGz4yn73o9klbF0hv3Sw92Ft6/y5p91q7JwUAJJFG7Sj+5z//kcfjUc+ePSVJ//jHP/THP/5RixYtUr9+DXuYMP/VAACAY6goMe9jnP+cVLo5ul58unnwzYnnSh6fbeMBQEuhDZyr2Zee5uXl6Y9//KOuv/76w74eCAQUCATqPy8rK1NRURE/DAAAHEskLK2dIc17WlrzoaTa/y87q5M09Gpp6GQp5zhbRwSA5iAUnatRl57GCofDevnll1VZWanRo0cf8X1Tp05VTk5O/UdRUVFTvyUAAMnF7ZFOOFu64t/ST5ZIp94uZXSUyrdLs++XHh4gvXyFtO4jKRKxe1oAQAJp9I7ismXLNHr0aFVXVyszM1Mvvviizj333CO+nx1FAABaUCgorXxbmv+MtOnT6Hped2n4ddLgK6T0PPvmA4BGYEfRuRodisFgUJs3b9b+/fv12muv6e9//7tmz56tvn37Nuj388MAAEALKVlpBuOSl6VAmbnm8ZsnpQ6/Xuoy3DwsBwAcijZwrmbfozh+/Hj16NFDTzzxRIPezw8DAAAtLFAhff2qeS/jjqXR9cIBZjAOuETyZ9o3HwAcAW3gXE2+R7GOYRiWS0sBAEAb82dKw66Rbpwj/WCmNOhyyZsq7Vgm/XeK+YiNd+4wdyABAGgAb2PefNddd+k73/mOioqKVF5erpdfflmzZs3S+++/31rzAQCAhnK5zMtNuwyXzv6DtPhF89LUveukeU+ZH11PNh+x0ecCyeu3e2IAgEM1KhR37typq666Stu3b1dOTo4GDhyo999/XxMmTGit+QAAQFOk50kn3yqNulnaMFua/7S06l1p82fmR3oHaehV0rBrpXbH2z0tAMBhmn2PYmNxHTIAADYp2yYt/Ke04DnzERuSJJfUa4J5L2OvCeYjOQCgjdAGzkUoAgCQbMIhafV75uE36z+Orud0lYZNloZeLWXm2zcfgKRBGzgXoQgAQDLbs868j3HR81L1fnPN7TPvYRxxvXT8KTxiA0CroQ2ci1AEAABSzQFp+RvmLuPW+dH1jr2l4ddJgyZJqTn2zQcgIdEGzkUoAgAAq+1LzGBc9opUU2Wu+dKlAd8z72XsPNjW8QAkDtrAuQhFAABweNWl0pJp5ompu1ZF148bbl6W2u+7ki/NvvkAxD3awLkIRQAAcHSGIW36zAzGFW9LkRpzPTVXGnKleWlq+x62jgggPtEGzkUoAgCAhqsokRb9S5r/nFS6Obrefax5WeqJ50qeRj2mGUASow2ci1AEAACNFwlLa2eY9zKu+VBS7V8nsjpJQyebj9nI7mzriACcjzZwLkIRAAA0z75N0oLnzJ3Gyl3mmssjnfgd817G4rGS223jgACcijZwLkIRAAC0jFBQWvm2+VzGTZ9G1/O6m/cxDr5CSs+zbz4AjkMbOBehCAAAWl7JSjMYl7wsBcrMNY9f6n+ReS9jl+GSy2XvjABsRxs4F6EIAABaT6BC+vpVad7fpR3LouuFA8xgHHCJ5M+0bz4AtqINnItQBAAArc8wpG/nm4/Y+Pp1KRww1/3Z0sDvm/cy5vexd0YAbY42cC5CEQAAtK2qvdLiF8xLU/euj653PdkMxj4XSF6/ffMBaDO0gXMRigAAwB6RiLRhlvmIjW/ek4ywuZ7eQRp6lTTsWqnd8baOCKB10QbORSgCAAD7lW2TFvxDWvgPqXx77aJL6jXBvJex1wTJ7bF1RAAtjzZwLkIRAAA4R7jG3F2c/7S0flZ0PaerNPRqadD3pdyuto0HoGXRBs5FKAIAAGfavVZa8Ky06Hmpen90/fhTzANw+l4opeXaNR2AFkAbOBehCAAAnK3mgLT8DWnxi9LGuZJq/+ri8UsnniMNnCT1HC95U2wdE0Dj0QbORSgCAID4UfqttOwVack0adfK6HpantT/YnOnsctwyeWyb0YADUYbOBehCAAA4o9hSDuWmsG47BWpsiT6Wl4PMxgHXirlFds3I4Bjog2ci1AEAADxLRwyH7OxZJq06r9STVX0taJRZjD2+66UnmfbiAAOjzZwLkIRAAAkjkC5tPK/0tKXpfWzFb2fMUXqdZY0aJL5T6/f1jEBmGgD5yIUAQBAYirbJi17VVo6Tdr5dXQ9NVfqf5F5eWrRSO5nBGxEGzgXoQgAABLfjq/NXcZlr0rl26Pr7brV3s/4fal9D9vGA5IVbeBchCIAAEgekbC0YY65y7jibammMvpalxFmMPa7SMpob9+MQBKhDZyLUAQAAMkpWCmteseMxnUfSUbEXHd7zfsYB35fOuEcyZdq75xAAqMNnItQBAAAKN8pff2qtORl87Ebdfw5Ur+J5iE4RaMkt9u2EYFERBs4F6EIAAAQq2SlGYzLXpHKtkbXc7tKAy41o7FDL/vmAxIIbeBchCIAAMDhRCLSprnm8xlXvCUFy6OvdR5qBmO/i6TMjvbNCMQ52sC5CEUAAIBjCVZJ37wrLf23tHaGZITNdZdH6jleGvR96cRzJV+avXMCcYY2cC5CEQAAoDEqdklfv2Y+bmPbouh6SpbU90IzGo8/lfsZgQagDZyLUAQAAGiqXavNYFz6b6l0S3Q9u4s08BJp4CQpv7d98wEORxs4F6EIAADQXJGItPlzMxqXvyUFSqOvdRpkPmqj//ekrAL7ZgQciDZwLkIRAACgJdVUS6vfMw/BWTtdioTMdZdH6jHO3GXsfZ6Ukm7vnIAD0AbORSgCAAC0lso90vLXzcdtbJ0fXU/JlPr8P2ngpVLxGMntsW9GwEa0gXMRigAAAG1h91pp6TTzY/+m6HpWZ2nA98zHbRT0s28+wAa0gXMRigAAAG3JMKQtX5q7jMvfkKr3R18rGGCemtr/e1J2J9tGBNoKbeBchCIAAIBdQgFp9QfmLuPqD6RIjbnuckvFp5u7jL3Pl/yZ9s4JtBLawLkIRQAAACeo2mvuMC6dZu441vGlS30uMO9n7D6O+xmRUGgD5yIUAQAAnGbvemnpK+bjNvauj65nFkgDLjEft1E4QHK57JsRaAG0gXMRigAAAE5lGNK3881g/Po16cC+6Gv5fc1dxgGXSjnH2Tcj0Ay0gXMRigAAAPEgFDSfy7h0mvTNe1I4GH2tYID5mI3iMdLxJ0up/B0L8YE2cC5CEQAAIN4c2C+teFNaMk3a/Jn1NZdHOm5obTieLhWdJPnS7JgSOCbawLkIRQAAgHhWUSJt/ETaMEdaP1vat8H6usdvxmL3081w7DxE8vjsmRU4CG3gXIQiAABAItm/WdrwibRhthmP5dutr6dkmZen1l2qWtBfcrvtmRVJjzZwLkIRAAAgURmGtGettH6WGY0bP7EeiCNJaXlS8Wm14ThWat+D01TRZmgD5yIUAQAAkkUkIu1cZkbjhjnSps+kYIX1PVmdzWjsfrr5z5wu9syKpEAbOBehCAAAkKzCNdLWhbXhOFva8qX1NFVJyutu3ttYd6lqRgd7ZkVCog2ci1AEAACAqeaAGYvra+9v3LZQMiLW9xT0j3kUxyk8igPNQhs4F6EIAACAw6suNS9PrbtUdefX1tddHvMU1bpLVYtG8igONApt4FyEIgAAABqmYlf0URwb5kh711lf96SYsVh3qepxQ3kUB46KNnAuQhEAAABNs3+L9RmO5dusr6dkHvQojgE8igMWtIFzEYoAAABoPsOQ9qyrfX7jbPNZjgf2Wt+T1k7qVvsoju5jpfY9eRRHkqMNnItQBAAAQMuLRKSS5dGDcTZ9ephHcXSq3W2svVQ1t8ieWWEb2sC5CEUAAAC0vnCNtG1R7W7jHGnzl1I4YH1Pu+Lo8xu7jZEyO9ozK9oMbeBchCIAAADaXs0BactX0Wc4bl0oGWHre/L7Re9v7HaKlJpjz6xoNbSBcxGKAAAAsF91mbT58+ilqjuXWV93uaOP4iiufRRHSro9s6LF0AbORSgCAADAeSr3SBvnRB/FsWet9XVPitTlpOilqscN41EccYg2cC5CEQAAAM5X+q15kmrdpaplW62v+zKsj+IoHMijOOIAbeBchCIAAADii2FIe9dHD8bZMEeq2mN9T1o78xLVXmdJPcdLWQX2zIqjog2ci1AEAABAfItEpJIV0d3GjZ9KwXLrezoNNqPxhLPNex3dHltGhRVt4FyEIgAAABJLOGQ+imPtdGnNh+avY6W3N3cZe50l9ThDSs+zZ07QBg5GKAIAACCxVZRIa2dIqz+Q1n0sBUqjr7nc5qE4vSaYu40F/SWXy75Zkwxt4FyEIgAAAJJHuMZ8fuOaD82PkhXW17M6mdHY62zzRFV/lj1zJgnawLkIRQAAACSv/VvMS1RXf2je31hTFX3N7TNPUq27t7F9T3YbWxht4FyEIgAAACBJNdXSpk+lNdOlNR+YJ6vGatfNjMZeZ0ndTpV8abaMmUhoA+ciFAEAAIDD2bMueonqxrlSOBh9zZtmPq+x1wQzHNsdb9+ccYw2cC5CEQAAADiWQIX5+I26cCzban29Y+9oNHYdLXl89swZZ2gD5yIUAQAAgMYwDPMQnDUfmpepbv5CMsLR11OypB7jai9TnSBlFdo3q8PRBs5FKAIAAADNcWCf+diNNdPNg3Eqd1lf7zQoem/jccMkt8eeOR2INnAuQhEAAABoKZGItH1R7YE4H0pbF0qK+et2Wp7Uc7wZjT3PlNLzbBvVCWgD52pUKE6dOlWvv/66Vq1apbS0NJ188sm6//77deKJJzb4G/LDAAAAgKRRsUtaO8OMxnUzperS6Gsut9RlRPTexsKBSff4DdrAuRoViuecc44mTZqkESNGKBQK6e6779ayZcu0YsUKZWRkNOhr8MMAAACApBQOSd9+Fb23cefX1tczC6Ve46VeZ0vdx0qpif93ZdrAuZp16emuXbuUn5+v2bNna8yYMQ36PfwwAAAAAJJKt0ajcf0sqaYy+prba56eesLZ5m5jhxMScreRNnAub3N+c2mpuXWel3fka6sDgYACgUD952VlZc35lgAAAEBiyDlOGn6t+REKSJs+iz5+Y89aaeMn5seHv5Jyu5o7jb3OkrqdKqWk2z09ElyTdxQNw9CFF16offv26ZNPPjni++655x7de++9h6zzXw0AAACAI9izLnogzsa5Uji68SJvqtTttNrdxglSu262jdlc7Cg6V5ND8ZZbbtE777yjuXPnqkuXLkd83+F2FIuKivhhAAAAABoiWClt+ERa84EZj6VbrK93OCH6+I2uoyVvij1zNgGh6FxNCsXbbrtNb775pubMmaPi4uJG/V5+GAAAAIAmMgxp1ypzp3H1h9LmzyUjHH09JdM8CKcuHLM72TZqQ9AGztWoUDQMQ7fddpveeOMNzZo1S7169Wr0N+SHAQAAAGghB/abB+HUXaZaWWJ9vXBAbTSeLXUZLrk9dkx5RLSBczUqFG+++Wa9+OKLeuuttyzPTszJyVFaWlqDvgY/DAAAAEAriESkHUvMaFz9gbR1gaSYv+qntZN6nGmGY8/xUkZ720atQxs4V6NC0XWEI3mfffZZXXPNNQ36GvwwAAAAAG2gcre0dqZ5b+PamVL1/uhrY34unXG3baPVoQ2cq1GPx2jGIxcBAAAAtKWMDtKg75sf4ZC0dX708Ru9zrJ7Ojhck089bSr+qwEAAAAAiTZwMrfdAwAAAAAAnIVQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFgQigAAAAAAC0IRAAAAAGBBKAIAAAAALAhFAAAAAIAFoQgAAAAAsCAUAQAAAAAWhCIAAAAAwIJQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFgQigAAAAAAC0IRAAAAAGBBKAIAAAAALAhFAAAAAIAFoQgAAAAAsCAUAQAAAAAWhCIAAAAAwIJQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFgQigAAAAAAC0IRAAAAAGBBKAIAAAAALAhFAAAAAIAFoQgAAAAAsEjqUDQMw+4RAAAAAMBxGh2Kc+bM0QUXXKDOnTvL5XLpzTffbIWxWl95dY1Ovf9j/eK1pZqxYqcOBMN2jwQAAAAAjuBt7G+orKzUoEGDdO211+riiy9ujZnaxJzVu7V1/wG9PG+LXp63Rak+t07t2VET+ubrjN4F6pjlt3tEAAAAALCFy2jG9Zcul0tvvPGGJk6c2ODfU1ZWppycHJWWlio7O7up37rZgqGIvtywRzNW7NSMlSXauv9A/WsulzS4KFfj+xRoQt8C9crPlMvlsm1WAAAAIBE5pQ1wqFYPxUAgoEAgUP95WVmZioqKHPXDYBiGVm4v14yVOzVj5U4t/bbU8nrXvHSN71Og8X3zNaJbnnyepL61EwAAAGgRhKJztXoo3nPPPbr33nsPWXfyD8POsmrNXFmiGSt3au7a3QqGIvWvZad6Na53vsb3KdDpJ3ZUdqrPxkkBAACA+EUoOhc7isdQFQzpkzW7NWPFTn20qkR7KoP1r3ndLo3q3l5n9jHDsSgv3cZJAQAAgPhCKDpXow+zaSy/3y+/P34PhklP8ersfoU6u1+hwhFDi7fs0/QV5m7j2pIKzV27W3PX7ta9/1mh3oVZtZeoFmjgcTlyu7mvEQAAAED8afVQTCQet0vDjs/TsOPz9Ivv9NaG3ZWauXKnpq/YqXkb92rVjnKt2lGuRz5eq45Zfo2v3Wk8pWcHpfo8do8PAAAAAA3S6EtPKyoqtHbtWknSkCFD9NBDD2ncuHHKy8tT165dj/n7E3V7eV9lULNWl2jGihLNXr1LFYFQ/WupPrdO69VRE/oUaFzvfB69AQAAAChx2yARNDoUZ82apXHjxh2yPnnyZD333HPH/P3J8MMQCIX15fq95imqK3ZqW2l1/WsulzSkKFfj+xZoQp8C9eTRGwAAAEhSydAG8apZh9k0RbL9MBiGoRXbyzRjRYlmrjr00RvHt6999EafAo3o1k5eHr0BAACAJJFsbRBPCMU2tqO0WjNXmTuNn67bY3n0Rk6aT+NO7KjxfQs05gQevQEAAIDEluxt4GSEoo0qA7WP3lhpPnpjb8yjN3wel0YWt9f4Pvk6k0dvAAAAIAHRBs5FKDpEOGJo0eZ9ml57X+O6XZWW13sXZmlCX/MS1QE8egMAAAAJgDZwLkLRodbvqtDMlSWavnKn5m/cq0jM/yvlZ/l1Zp8CTeibr5N78OgNAAAAxCfawLkIxTiwrzKoj78p0YyVOzX7m12qDIbrX0vzeXRarw4a37dAZ/TOV4dMHr0BAACA+EAbOBehGGcCobC+WL9XM1bs1IyVO7X9oEdvDO3aTuNrdxt7dOTRGwAAAHAu2sC5CMU4ZhiGlm8r08yV5m7jsq3WR290q3v0Rt8CDT+eR28AAADAWWgD5yIUE8j20gP10fjZ2j0Khq2P3jijd77G9ynQmBM6KItHbwAAAMBmtIFzEYoJqiIQ0tw1uzR9RYk+WrVT+6pq6l/zeVwa1b29xvcp0Jl98tWlHY/eAAAAQNujDZyLUEwC4YihhZv3acaKnZq+cqfWH/TojT6dsjWhT77G9y1Q/848egMAAABtgzZwLkIxCa3bVaGZK3dqxooSzd9kffRGQbb56I3TenZQfrZfOWkpyk33KSfNJx/3OAIAAKAF0QbORSgmub2VQX28qvbRG6t3qSrm0RsHy/R7lZPmqw9H859mSOYe/Hm6T7m1v+Y5jwAAADgc2sC5CEXUq64J64v1ezRj5U4t/bZU+6tqtL8qqLLqULO+rt/rrg/HnLrIrA3L3PSU+ujMjdm9zE33KdPv5fEeAAAACYw2cC5CEccUjhgqO1Cj0gM12n/AjMfSAzW1IVmj/QeCKq2Kvrb/QE395+FI03+8PG6XctN8yqndsTQDMjYsaz+P+XVumk/ZaT55uM8SAADA8WgD5/LaPQCcz+N2qV1GitplpDTq9xmGocpg2IzHqppoXB6I/TxYu1YXl+bngVBE4YihPZVB7akMNnrm7FSvGY61O5SH37WMXjZbt9Pp93KZLAAAAEAootW4XC5l+r3K9HvVpV3jfm91TThm1zJ4SEge8nlVjcoO1Kg8YF4mW1YdUll1SJv3Nu77pqd4asMxJWYXs25X0wzLrFTzz2T+06fM2s8z/V52MgEAAJAQCEU4UqrPo1SfRwXZqY36fTXhiMrqL5GtUWlMSJpxGax/bf8BMy7rLqWNGFJVMKyqYFjbSqubNHd6iseMxlSvsmr/aUZkNDAzLaHpjXl/NDpTvJwwCwAAAPsQikgoPo9b7TP9ap/pb9Tvi0QMlQdCh9m1jP66LjzLq0OqCNR+VIdUHggpGIpIioZmSXmgWX+OFK/7oND0HhSavsOEZsz7a8Mz1efmQCAAAAA0GqEISHK7XfX3MnZVeqN/fyAUVmUgXBuONaqIicn6sKyO/bzGEpp1r9U9niQYimhPqGn3Z8byuF2HhGZGbFgessPpO3THM9WrjBQuqwUAAEgmhCLQAvxej/xej/IaeeDPwULhiCqD4ZiwrDlCaMZ8Hgipovqg8AyEZBjmibWltSfWNldGiidmx9J3hND0KivVjM2smF3OrFQf93ECAADEEUIRcBCvx62cNLdy0nzN+jqGYaiqNjitYXmY8Iz5dTQ0zV3R8uqQQrWPOKkMhlUZDGunmndZbV1w1sVjVupBkWlZq73ENtWr7Jgdz4wUD5fUAgAAtCJCEUhALpdLGbWXmRY045FEhmEoEIocfkezLibrArM6pMqAedrswUFaXh1SMGzex1kfnGVND063S8rwe5Vdt1N5UGxmHWaX07KWav5ev5d7OAEAAA6HUARwRC6Xq/4E2g6NPCDoYIFQ2IzH2oAsq47uWtbdz3nIWnXtWkyghiOGIoZUXvu+5vC6XTGR6au/XLYuJuvXYt4Tu+NZtwPKKbUAACDREIoA2oTf65E/s3nBaRiGqmsiKq82n5kZDc8acyez2npYUHStxrK7WRE07+EMRYz6x6dIB5o8V4rXXXtpbMz9mDH3aeZl+NUhK0XtM/zqmJWiDrUn83IJLQAAcCpCEUDccLlcSkvxKC3Fo/xmfJ1IxFBVTdgMyOq6y2WtUVkWE6Hl9bug0YODyqutp9Turghqd0XjTqlN9bnro7FjphmQ5ufRX3eo/XVOmk9uDgICAABthFAEkHTcMY8NUU7Tv04oHFFlIGw+EiVmh7MsZgezrLpGeyuD2lUe1J7KgHZXBLS7PKgDNWFV10T07b4D+nbfsXczvW6X8jJqAzIrGpB1/2xf++uOmX61y0iRz8PlsAAAoOkIRQBoIq/HrZx0t3LSG39KbWUgpD0VQe2qMONxT0XQjMjaX9et7y4PqKz29NmS8oBKygPS9mN//XbpvkN2Jztm+dU+JjbbZ6SoY5ZfqT5PE/70AAAgkRGKAGCDulNpu7ZPP+Z7g6GIuRtZHtTuSjMed1cEtacuJusjM6i9lQFFDGlfVY32VdVoTcmxZ8n0e2OCMqV2dzJ6OWzdbmWHLL+y/F7uqwQAIAkQigDgcCletzrlpKlTTtox3xuOGNpXFbTsUNaHZHlAeyqjv95dEVQwHKk/dXbTnqoGzdIhI6V+RzJ6KWzs5bDmTma79BR5uK8SAIC4RCgCQALxuF31sXaiso76XsMwVB4IHbJDueswu5V7KoKqCIQUDEW0rbRa20qrjzmL2yXzxNfD7Faav05RdqpPOWnmR3aaj8tgAQBwCEIRAJKUy+VSdqpP2ak+de947PcfCIbNaKwM1sal+etd5QHL/ZW7KwLaV1WjiKH6dam8QTOleN1mNKZ66wOyLiLrf50a/Tw7Lfq+TC6LBQCgxRCKAIAGSUvxqCgvXUV5x76vsiYc0b7KukN5Dn8/5b7KoEoP1KisukZlB8ywDIYi2lUe0K7yQKPnc7t0SFDWRWZsUMbuYta/nuqVl5NiAQCoRygCAFqcz+NWfnaq8rNTG/T+SMRQRTCksgM1Kq39KDtw0OfVsa/V/dp8TzAcUcSQ9lfVaH9VTZNmzvR7lZ3qjdmtPCgoU73KST80NrlkFgCQiAhFAIDt3O7oZbBd2jX+91fXhA8KyNqwrDJj8nChWffeymBYkuoP9WnI/ZcH45JZAECiIRQBAHEv1edRqs+jggbuYMYKhSMqqw4dPjSPsbvZWpfMZqV6len3KivVp8xUc6cz9vO63c/MVHMt3eeRmxNmAQAtiFAEACQ1r8etvIwU5WWkNPr3RiKGKoMhS1DW71jGhOahEWq+Lxhq/iWzkuRySZkpXjMwa+Mx0+89NDIta77o+2tfT/W52d0EAEgiFAEAaDK326WsVJ+ymnHJ7OF2KiuqQyqrNi+FLa82P68I1K5Vh1QeMNfKq0MKRQwZhlQeCKk8EJJKm/7n8bpd9TuWWak+ZfnrwjNmrf7XR1hL9crv5Z5NAIh3hCIAADapu2S2oYf+HMwwDAVCEZVX1wZlIFT7a2tkltevm++piHlPWe2aYUihiBGzu3mgyX+uFK/78JFpWTMDMyvm87rYrPs9nEQLAPYhFAEAiFMul6s+Njtm+Zv8dQzDUFUwXBuPNdGdy9rPD4nPIwRp3cFAwVBEe0JB7akMNuvPl+bz1Idl1kGXz1rWLEEafV9WqlcZKV7u3wSAJiAUAQBIci6XSxl+rzL8XklN292UpHDEqD89tn43s35HM3oJbV1k1u9wHrQWCEUkSQdqwjpQE27SIUHRP5t5/2ZsTGbWXi6b5bfubmYeJj65fxNAsiIUAQBAi/C4XfWnt0ppTf46wVAkeomsZUfTev9mRUxs1u2C1kXq4e7f3N6M+zc9blc0NP1eZR+yk3nk+zejgcr9mwDiB6EIAAAcJcXrVp63aSfR1jnc/ZuHOySo7v7Ng3dBYy+zjRjmbmlzT6et+7NlxRz8k+W37l5mHnTpbOyltHWBmuH3cP8mgFZHKAIAgITT2vdvxu5c2nH/ZnqKJ+akWV/MY1AOPSgo66Bnc9Y9gzPN5+FyWgBHRCgCAAAcQVvcvxl7CW1D79+sCoZVFQyrpBn3b3rcLssBQFmxp87GBGbsQUKx4cnptEBiIxQBAABaWUvev1lZF48H3b8ZDcqDTqutNh+DErvLWXc5bd0zPJvzOJQ0nycmMK27m7GBGXtfZ906u5uAcxGKAAAAcSLF61aKN0Xtmnn/5oGacP0u5eECM/qIlNidzhrLMzmra6yn0zZ3d/OQQ4D8R97dPNKBQexuAi2HUAQAAEgiLpdL6Slepad4VZDd9Mtpa8KR+vs163csD9rpLD8oNo+2uxk9LKjldjcz/R6l+bxKT/EozedRWopH6bUfaSne6K9jXqt7v/kej9JTvPLwLE4kIUIRAAAAjebzuNUuo/m7m1XBcH00RncyD7+7GRuYrbW7eTgpXrfSfLHx6FG6zxsTlzGheVCApqd4lZbiPmyApqd45PfyjE44E6EIAAAAW8QeFtQSu5sH37tZFQzVH/xzIBjSgZq6X4ej6zWh+jXr6+ajUSTz3tBgKFJ7P2fLcrlkjdCYAE1PMU/uTU+pC06P0n3W0Iy+bl1Pqw1VH5fjookIRQAAAMS1ltjdPFjdszgPBMOqqqkNzWDEjM+aaGweiI3R2PWa2Eg1wzM2RutOsDWM6Cm2rcHncdXvbMbuiF5/arHO6d+pVb4nEgOhCAAAABwk9lmc7Vrh64cjRm00hqw7nPUxGYqJznD97ujhdj6rjrIbWhM2VBM2L+GNNXHIca3wp0IiIRQBAACANlb3HMtMf8v/ddwwDAXDkUMCNHY3tH/nnBb/vkgshCIAAACQQFwul/xej/xej3LT7Z4G8Yq7WwEAAAAAFoQiAAAAAMCCUAQAAAAAWBCKAAAAAAALQhEAAAAAYEEoAgAAAAAsCEUAAAAAgAWhCAAAAACwIBQBAAAAABaEIgAAAADAglAEAAAAAFgQigAAAAAAC0IRAAAAAGBBKAIAAAAALAhFAAAAAIAFoQgAAAAAsCAUAQAAAAAW3rb+hoZhSJLKysra+lsDAAAAcJC6JqhrBDhHm4dieXm5JKmoqKitvzUAAAAAByovL1dOTo7dYyCGy2jjfI9EItq2bZuysrLkcrna8lsfoqysTEVFRdqyZYuys7NtnQXxgZ8ZNBY/M2gsfmbQGPy8oLGc9jNjGIbKy8vVuXNnud3cFeckbb6j6Ha71aVLl7b+tkeVnZ3tiP+hIH7wM4PG4mcGjcXPDBqDnxc0lpN+ZthJdCayHQAAAABgQSgCAAAAACySOhT9fr/+53/+R36/3+5RECf4mUFj8TODxuJnBo3Bzwsai58ZNFSbH2YDAAAAAHC2pN5RBAAAAAAcilAEAAAAAFgQigAAAAAAC0IRAAAAAGBBKAIAAAAALJI2FB977DEVFxcrNTVVw4YN0yeffGL3SHCoqVOnasSIEcrKylJ+fr4mTpyob775xu6xEEemTp0ql8ulKVOm2D0KHGzr1q268sor1b59e6Wnp2vw4MFasGCB3WPBoUKhkH71q1+puLhYaWlp6t69u377298qEonYPRocYs6cObrgggvUuXNnuVwuvfnmm5bXDcPQPffco86dOystLU1jx47V8uXL7RkWjpSUoTht2jRNmTJFd999txYtWqTTTjtN3/nOd7R582a7R4MDzZ49W7fccou++OILTZ8+XaFQSGeddZYqKyvtHg1xYN68eXryySc1cOBAu0eBg+3bt0+nnHKKfD6f3nvvPa1YsUIPPvigcnNz7R4NDnX//ffrb3/7mx555BGtXLlSDzzwgP74xz/qr3/9q92jwSEqKys1aNAgPfLII4d9/YEHHtBDDz2kRx55RPPmzVNhYaEmTJig8vLyNp4UTpWUz1EcOXKkhg4dqscff7x+rU+fPpo4caKmTp1q42SIB7t27VJ+fr5mz56tMWPG2D0OHKyiokJDhw7VY489pt///vcaPHiwHn74YbvHggP94he/0KeffsrVLWiw888/XwUFBXr66afr1y6++GKlp6frX//6l42TwYlcLpfeeOMNTZw4UZK5m9i5c2dNmTJFd955pyQpEAiooKBA999/v2688UYbp4VTJN2OYjAY1IIFC3TWWWdZ1s866yx99tlnNk2FeFJaWipJysvLs3kSON0tt9yi8847T+PHj7d7FDjc22+/reHDh+uSSy5Rfn6+hgwZoqeeesruseBgp556qmbOnKnVq1dLkpYsWaK5c+fq3HPPtXkyxIMNGzZox44dlr8P+/1+nX766fx9GPW8dg/Q1nbv3q1wOKyCggLLekFBgXbs2GHTVIgXhmHo9ttv16mnnqr+/fvbPQ4c7OWXX9bChQs1b948u0dBHFi/fr0ef/xx3X777brrrrv01Vdf6cc//rH8fr+uvvpqu8eDA915550qLS1V79695fF4FA6H9Yc//EGXXXaZ3aMhDtT9nfdwfx/etGmTHSPBgZIuFOu4XC7L54ZhHLIGHOzWW2/V0qVLNXfuXLtHgYNt2bJFP/nJT/Thhx8qNTXV7nEQByKRiIYPH6777rtPkjRkyBAtX75cjz/+OKGIw5o2bZqef/55vfjii+rXr58WL16sKVOmqHPnzpo8ebLd4yFO8PdhHE3ShWKHDh3k8XgO2T0sKSk55L+qALFuu+02vf3225ozZ466dOli9zhwsAULFqikpETDhg2rXwuHw5ozZ44eeeQRBQIBeTweGyeE03Tq1El9+/a1rPXp00evvfaaTRPB6X72s5/pF7/4hSZNmiRJGjBggDZt2qSpU6cSijimwsJCSebOYqdOnerX+fswYiXdPYopKSkaNmyYpk+fblmfPn26Tj75ZJumgpMZhqFbb71Vr7/+uj766CMVFxfbPRIc7swzz9SyZcu0ePHi+o/hw4friiuu0OLFi4lEHOKUU0455LE7q1ev1vHHH2/TRHC6qqoqud3Wv8Z5PB4ej4EGKS4uVmFhoeXvw8FgULNnz+bvw6iXdDuKknT77bfrqquu0vDhwzV69Gg9+eST2rx5s370ox/ZPRoc6JZbbtGLL76ot956S1lZWfW70Tk5OUpLS7N5OjhRVlbWIfewZmRkqH379tzbisP66U9/qpNPPln33XefLr30Un311Vd68skn9eSTT9o9Ghzqggsu0B/+8Ad17dpV/fr106JFi/TQQw/puuuus3s0OERFRYXWrl1b//mGDRu0ePFi5eXlqWvXrpoyZYruu+8+9erVS7169dJ9992n9PR0XX755TZODSdJysdjSNJjjz2mBx54QNu3b1f//v315z//mUcd4LCOdK3+s88+q2uuuaZth0HcGjt2LI/HwFH997//1S9/+UutWbNGxcXFuv3223XDDTfYPRYcqry8XL/+9a/1xhtvqKSkRJ07d9Zll12m3/zmN0pJSbF7PDjArFmzNG7cuEPWJ0+erOeee06GYejee+/VE088oX379mnkyJF69NFH+Q+aqJe0oQgAAAAAOLyku0cRAAAAAHB0hCIAAAAAwIJQBAAAAABYEIoAAAAAAAtCEQAAAABgQSgCAAAAACwIRQAAAACABaEIAAAAALAgFAEAAAAAFoQiAAAAAMCCUAQAAAAAWPz/tWInVtRp9DsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(train_loss, label='Train')\n",
    "ax.plot(dev_loss, label='Dev')\n",
    "fig.legend()\n",
    "fig.savefig('math_loss_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
