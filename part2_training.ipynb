{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:40:31.035566Z","iopub.status.busy":"2024-04-05T12:40:31.035094Z","iopub.status.idle":"2024-04-05T12:40:50.338593Z","shell.execute_reply":"2024-04-05T12:40:50.337593Z","shell.execute_reply.started":"2024-04-05T12:40:31.035529Z"},"trusted":true},"outputs":[],"source":["import torch\n","import json\n","import torch.nn as nn\n","import numpy as np\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import gensim #For word2vec\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","import time\n","import nltk\n","import random\n","from numpy.random import choice as randomchoice\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import sys\n","# import transformers\n","# from transformers import AutoModel, BertTokenizerFast\n","# import torchtext\n","import pickle\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import matplotlib.pyplot as plt\n","import matplotlib as mlt\n","import gensim.downloader as api\n","import torchtext.vocab as vocab"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:40:50.341303Z","iopub.status.busy":"2024-04-05T12:40:50.340401Z","iopub.status.idle":"2024-04-05T12:42:38.342839Z","shell.execute_reply":"2024-04-05T12:42:38.341820Z","shell.execute_reply.started":"2024-04-05T12:40:50.341270Z"},"trusted":true},"outputs":[],"source":["glove_model = api.load(\"glove-wiki-gigaword-200\")"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# bert = AutoModel.from_pretrained('bert-base-uncased')\n","\n","# Load the BERT tokenizer\n","# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:51.331780Z","iopub.status.busy":"2024-04-05T12:43:51.331438Z","iopub.status.idle":"2024-04-05T12:43:51.345338Z","shell.execute_reply":"2024-04-05T12:43:51.344410Z","shell.execute_reply.started":"2024-04-05T12:43:51.331756Z"},"trusted":true},"outputs":[],"source":["train_start_time = time.time();\n","# val_file = sys.argv[2];\n","train_file = 'data/train.json'\n","val_file = 'data/dev.json'\n","test_file = 'data/test.json'\n","\n","tokenize_func = nltk.tokenize.WordPunctTokenizer().tokenize\n","punctuations = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^`{|}~'\n","def is_numeric(s):\n","    try:\n","        float(s)\n","        return True\n","    except ValueError: #Classic way to get is_numeric\n","        return False\n","def tokenize(sentence, with_num = True):\n","    old = tokenize_func(sentence.lower());\n","    s = [];\n","    for word in old:\n","        running = [];\n","        for character in word:\n","            if(character in punctuations):\n","                if(len(running) > 0):\n","                    s.append(''.join(running));\n","                    running = []; #emptying the running list.\n","                s.append(character); #then adding the punctuation.\n","            else:\n","                running.append(character);\n","        if(len(running) > 0):\n","            s.append(''.join(running));\n","        #this above code ensures that what we have is also split on punctuation\n","    if(with_num):\n","        return s; #If with_num is true, return the sentence as it is, without converting the numbers to <NUM>\n","    for i in range(len(s)):\n","        if(is_numeric(s[i])):\n","            s[i] = '<NUM>'; #replaces numbers with <NUM>\n","    return s;\n","\n","def tokenize_with_num(sentence): #just tokenizes normally. No replacement of numbers\n","    s = tokenize_func(sentence.lower());\n","    return s;\n","\n","def get_embedding_index(sentences, model):\n","    return ([tokenize_and_get_embedding_index(sentence, model) for sentence in sentences]);\n","\n","def tokenize_and_get_embedding_index_as_list(sentence, vocab, with_num = False):\n","    s = tokenize(sentence, with_num = with_num);\n","    # FOr now testing with No UNK, Later will have to add UNK\n","    tens = ([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n","    return tens;\n","\n","def tokenize_and_get_embedding_index(sentence, vocab, with_num = False):\n","    s = tokenize(sentence, with_num = with_num);\n","    # FOr now testing with No UNK, Later will have to add UNK\n","    tens = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n","    return tens;\n","    if(len(tens) == 0):\n","        return torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) #using UNK in this case.\n","    else:\n","        return tens;"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["['const_100']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tokenize(\"const_100\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:54.112258Z","iopub.status.busy":"2024-04-05T12:43:54.111876Z","iopub.status.idle":"2024-04-05T12:43:54.263387Z","shell.execute_reply":"2024-04-05T12:43:54.262611Z","shell.execute_reply.started":"2024-04-05T12:43:54.112229Z"},"trusted":true},"outputs":[],"source":["with open(train_file) as f:\n","    train_data = json.load(f)\n","with open(val_file) as f:\n","    val_data = json.load(f)\n","with open(test_file) as f:\n","    test_data = json.load(f)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:55.148651Z","iopub.status.busy":"2024-04-05T12:43:55.148279Z","iopub.status.idle":"2024-04-05T12:43:55.152800Z","shell.execute_reply":"2024-04-05T12:43:55.151879Z","shell.execute_reply.started":"2024-04-05T12:43:55.148626Z"},"trusted":true},"outputs":[],"source":["global_max_len = 120;"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:56.678057Z","iopub.status.busy":"2024-04-05T12:43:56.677705Z","iopub.status.idle":"2024-04-05T12:43:56.696994Z","shell.execute_reply":"2024-04-05T12:43:56.696111Z","shell.execute_reply.started":"2024-04-05T12:43:56.678031Z"},"trusted":true},"outputs":[],"source":["## Postprocessing the code to remove the last '|' that is sometimes randomly present.\n","def remove_last_extra(data):\n","    for i in range(len(data)):\n","        if(data[i]['linear_formula'][-1] == '|'):\n","            data[i]['linear_formula'] = data[i]['linear_formula'][:-1];\n","        \n","    return data; #although not really needed.\n","remove_last_extra(val_data);\n","remove_last_extra(train_data);\n","remove_last_extra(test_data);"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:58.474298Z","iopub.status.busy":"2024-04-05T12:43:58.473850Z","iopub.status.idle":"2024-04-05T12:43:58.483046Z","shell.execute_reply":"2024-04-05T12:43:58.482165Z","shell.execute_reply.started":"2024-04-05T12:43:58.474262Z"},"trusted":true},"outputs":[],"source":["class glove_vectors():\n","    def get_word_embedding(word, glove_vectors, dim):\n","        if word in glove_vectors.key_to_index: #if the key is present we initialize it as glove embedding\n","            return torch.tensor(glove_vectors[word])\n","        else:\n","            return torch.rand(dim)  # Initi\n","    def __init__(self, sentences, glove_model, dim=200):\n","        self.vocabulary = set(['<START>', '<END>', '<PAD>', '<UNK>', '<NUM>']);\n","        for sentence in sentences:\n","            for word in tokenize(sentence):\n","                self.vocabulary.add(word); #creates the vocabulary.\n","        self.word_to_index = {word: i for i, word in enumerate(self.vocabulary)};\n","        self.index_to_word = {i: word for i, word in enumerate(self.vocabulary)};\n","        self.wordvec = [0] * len(self.vocabulary); #initializing the encoder_wordvec list\n","        rand_count = 0;\n","        for i in range(len(self.vocabulary)):\n","            self.wordvec[i] = glove_vectors.get_word_embedding(self.index_to_word[i], glove_model, dim);\n","        self.wordvec = torch.stack(self.wordvec); #stacking the list of tensors to form a tensor."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:43:59.342974Z","iopub.status.busy":"2024-04-05T12:43:59.341951Z","iopub.status.idle":"2024-04-05T12:44:03.198029Z","shell.execute_reply":"2024-04-05T12:44:03.197194Z","shell.execute_reply.started":"2024-04-05T12:43:59.342933Z"},"trusted":true},"outputs":[],"source":["encoder_vectors = glove_vectors([data['Problem'] for data in train_data], glove_model);\n","decoder_vectors = glove_vectors([data['linear_formula'] for data in train_data], glove_model);"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:03.199881Z","iopub.status.busy":"2024-04-05T12:44:03.199587Z","iopub.status.idle":"2024-04-05T12:44:03.217049Z","shell.execute_reply":"2024-04-05T12:44:03.216195Z","shell.execute_reply.started":"2024-04-05T12:44:03.199856Z"},"trusted":true},"outputs":[],"source":["with open('mysaved.pkl', 'wb') as f:\n","    pickle.dump([encoder_vectors, decoder_vectors], f);"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:07.272268Z","iopub.status.busy":"2024-04-05T12:44:07.271528Z","iopub.status.idle":"2024-04-05T12:44:07.285318Z","shell.execute_reply":"2024-04-05T12:44:07.284405Z","shell.execute_reply.started":"2024-04-05T12:44:07.272233Z"},"trusted":true},"outputs":[],"source":["class LSTM_on_words(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index,bidirectional=True, dropout=0.0):\n","        super(LSTM_on_words, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional).to(device);\n","\n","    def forward(self, x, x_lengths):\n","        # Embedding\n","        out = self.embedding(x)\n","        # Pack padded sequence\n","        # lengths = x_lengths.detach().cpu().numpy();\n","        out = pack_padded_sequence(out, x_lengths, batch_first=True, enforce_sorted=False).to(device);\n","        out, (hidden, cell) = self.lstm(out)\n","        # Unpack packed sequence\n","        out, _ = pad_packed_sequence(out, batch_first=True)\n","        return out, (hidden, cell);\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, input_size, layer_sizes):\n","        super(FeedForward, self).__init__()\n","        self.layers = [];\n","        self.ReLU = nn.ReLU(inplace=False)\n","        for i in range(len(layer_sizes)):\n","            if(i == 0):\n","                self.layers.append(nn.Linear(input_size, layer_sizes[i]));\n","            else:\n","                self.layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]));\n","            if(i != len(layer_sizes) - 1): #add Relu only if its not the last layer, since that is the output layer that we will softmax over.\n","                self.layers.append(self.ReLU);\n","        self.all_layers = nn.Sequential(*self.layers)\n","    def forward(self, x):\n","        out = self.all_layers(x)\n","        return out"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:08.111093Z","iopub.status.busy":"2024-04-05T12:44:08.110231Z","iopub.status.idle":"2024-04-05T12:44:08.121682Z","shell.execute_reply":"2024-04-05T12:44:08.120562Z","shell.execute_reply.started":"2024-04-05T12:44:08.111062Z"},"trusted":true},"outputs":[],"source":["class mathDataset(Dataset):\n","    def __init__(self, data, global_max_len = global_max_len):\n","        self.data = data;\n","        # self.encoder_word_to_index = encoder_wordvec.word_to_index;\n","        # self.vocab_index_to_word = vocab_index_to_word;\n","        self.max_len = global_max_len;\n","    def __len__(self):\n","        return len(self.data);\n","    def __getitem__(self, idx):\n","        problem = self.data[idx]['Problem'];\n","        linear_formula = self.data[idx]['linear_formula']; #maybe the linear formula can go directly without getting emebdded as well.\n","        problem = tokenize_and_get_embedding_index_as_list(problem, encoder_vectors.word_to_index);\n","        problem.append(encoder_vectors.word_to_index['<END>']);\n","        problem = torch.tensor(problem);\n","        linear_formula = tokenize_and_get_embedding_index_as_list(linear_formula, decoder_vectors.word_to_index);\n","        linear_formula.append(decoder_vectors.word_to_index['<END>']);\n","        #we need this linear formula to be of a constant size.\n","        padding_len = self.max_len - len(linear_formula)\n","        linear_formula = linear_formula[:self.max_len];\n","        if padding_len > 0:\n","            linear_formula += [decoder_vectors.word_to_index['<PAD>']] * padding_len\n","        linear_formula = torch.tensor(linear_formula)\n","        return problem, linear_formula;\n","\n","def collate_fn(data):\n","    # data.sort(key=lambda x: len(x[0]), reverse=True)\n","    problems, linear_formulas = zip(*data)\n","    # problems = data; #zip(*data)\n","    problems_lengths = [len(problem) for problem in problems]\n","    # linear_formulas = pad_sequence\n","    problems = pad_sequence(problems, batch_first=True, padding_value=encoder_vectors.word_to_index['<PAD>'])\n","    linear_formulas = pad_sequence(linear_formulas, batch_first=True, padding_value=decoder_vectors.word_to_index['<PAD>'])\n","    return problems, problems_lengths, linear_formulas;\n"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:09.806119Z","iopub.status.busy":"2024-04-05T12:44:09.805787Z","iopub.status.idle":"2024-04-05T12:44:09.811858Z","shell.execute_reply":"2024-04-05T12:44:09.810901Z","shell.execute_reply.started":"2024-04-05T12:44:09.806095Z"},"trusted":true},"outputs":[],"source":["train_dataset = mathDataset(train_data, global_max_len)\n","batch_size = min(32, len(train_dataset));\n","Train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn);\n","Dev_loader = DataLoader(mathDataset(val_data, global_max_len), batch_size=batch_size, shuffle=True, collate_fn=collate_fn);\n","Test_loader = DataLoader(mathDataset(test_data, global_max_len), batch_size=batch_size, shuffle=True, collate_fn=collate_fn);"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:13.095847Z","iopub.status.busy":"2024-04-05T12:44:13.095498Z","iopub.status.idle":"2024-04-05T12:44:13.124103Z","shell.execute_reply":"2024-04-05T12:44:13.123149Z","shell.execute_reply.started":"2024-04-05T12:44:13.095819Z"},"trusted":true},"outputs":[],"source":["#so our encoder is simply LSTM_on_words. Now to make the decoder LSTM_on_words.\n","teacher_forcing_probability = 0.9;\n","class Decoder_LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n","        super(Decoder_LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n","        self.fc = FeedForward(hidden_size,[len(wordvectors)*2,len(wordvectors)]).to(device);\n","        # self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n","    def forward(self, batch_size,max_len,encoder_outputs, hidden, cell, teacher_forcing = None):\n","        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n","        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n","        outputs = [];\n","        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n","        # print(\"teacher forcing:\", teacher_forcing.shape)\n","        for i in range(max_len):\n","            dec_out, (hidden, cell), _ = self.forward_step(dec_in,hidden, cell); #we get the value after one step of the LSTM.\n","            outputs.append(dec_out);\n","            # print(dec_out.shape);\n","            if(teacher_forcing == None):\n","                _, ind = dec_out.topk(1);\n","                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n","            else:\n","                if(random.random() > teacher_forcing_probability):\n","                    _, ind = dec_out.topk(1);\n","                    dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n","                else:\n","                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n","                    dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n","        outputs = torch.cat(outputs, dim=1)\n","        return outputs, (hidden, cell), []; #same format as the attention one.\n","    \n","    def forward_step(self, inputs, hidden, cell):\n","        outs = self.embedding(inputs);\n","        outs, (h, c)  = self.lstm(outs, (hidden, cell));\n","        outs = self.fc(outs); #\n","        return outs, (h, c), []; #same format as attention one.\n","\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, input_size, max_len):\n","        super(Attention, self).__init__()\n","        self.input_size = input_size;\n","        self.max_len = max_len;\n","        self.attn_calc = FeedForward(input_size, [input_size//2, 1]).to(device); # a simple 3 layer feed forward network. For fast training.\n","        self.KeyMatrix = nn.Linear(input_size, input_size).to(device);\n","        self.ValueMatrix = nn.Linear(input_size, 1).to(device);\n","        self.QueryMatrix = nn.Linear(input_size, input_size).to(device);\n","    \n","    def forward(self, hidden, encoder_outputs): \n","        out = self.ValueMatrix(torch.tanh(self.QueryMatrix(hidden) + self.KeyMatrix(encoder_outputs)));\n","        out = out.squeeze(2).unsqueeze(1);\n","        weights = F.softmax(out, dim=-1);\n","#         print(weights.shape, encoder_outputs.shape);\n","        #need to take care of this batch matrix multiplication by multiplying with another linear embedding that reduces the dimensionality back to 200 instead of the concatenated 400.\n","        #or maybe we can just use 400 here? and keep hidden size of weights as 400 too?.\n","        context = torch.bmm(weights, encoder_outputs); \n","        return context, weights;\n","\n","#now to use a decoder with attention we will need to do the following.\n","class Decoder_LSTM_with_attention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n","        super(Decoder_LSTM_with_attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.attention = Attention(self.hidden_size,global_max_len)\n","        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=False).to(device);\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n","        self.fc = FeedForward(hidden_size,[len(wordvectors)*2,len(wordvectors)]).to(device);\n","        # self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n","    \n","    def forward(self, batch_size,max_len,encoder_outputs, hidden, cell, teacher_forcing = None):\n","        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n","        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n","        outputs = [];\n","        attentions = [];\n","        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n","        # print(\"teacher forcing:\", teacher_forcing.shape)\n","        for i in range(max_len):\n","            dec_out, (hidden, cell), attention_weights = self.forward_step(dec_in,hidden, cell, encoder_outputs); #we get the value after one step of the LSTM.\n","            outputs.append(dec_out);\n","            attentions.append(attention_weights);\n","            # print(dec_out.shape);\n","            if(teacher_forcing == None):\n","                _, ind = dec_out.topk(1);\n","                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n","            else:\n","                if(random.random() > teacher_forcing_probability):\n","                    _, ind = dec_out.topk(1);\n","                    dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n","                else:\n","                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n","                    dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n","        outputs = torch.cat(outputs, dim=1)\n","        return outputs, (hidden, cell), attentions\n","    \n","    def forward_step(self, inputs, hidden, cell, encoder_outputs):\n","        outs = self.embedding(inputs);\n","        query = hidden.permute(1,0,2);\n","        context, attention_weights = self.attention(query, encoder_outputs);\n","        inp = torch.cat((outs, context), dim=2); #concatenating them.\n","#         print(\"input is: \", outs.shape, \" + \", context.shape, \" = \", outs.shape[2] + context.shape[2]);\n","        outs, (h, c)  = self.lstm(inp, (hidden, cell));\n","        outs = self.fc(outs); #passing through feedforward.\n","        return outs, (h, c), attention_weights"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:44:15.773960Z","iopub.status.busy":"2024-04-05T12:44:15.773105Z","iopub.status.idle":"2024-04-05T12:44:15.781199Z","shell.execute_reply":"2024-04-05T12:44:15.780242Z","shell.execute_reply.started":"2024-04-05T12:44:15.773924Z"},"trusted":true},"outputs":[],"source":["def load_checkpoint(model, optimizer, filename):\n","    checkpoint = torch.load(filename);\n","    model.load_state_dict(checkpoint['model_state_dict']);\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict']);\n","    epoch = checkpoint['epoch'];\n","    loss = checkpoint['loss'];\n","    encoder_vectors = None; decoder_vectors = None;\n","    try:\n","        encoder_vectors, decoder_vectors = checkpoint['word2vec']; #to get back the word2vec files.\n","    except:\n","        print(\"encoder vectors not stored in checkpoint.\");\n","    return model, optimizer, epoch, loss, encoder_vectors, decoder_vectors;\n","\n","def store_checkpoint(model, optimizer, epoch, loss, filename, encoder_vectors = None, decoder_vectors=None):\n","    torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'epoch': epoch,\n","            'loss': loss,\n","            'word2vec': (encoder_vectors, decoder_vectors),\n","            }, filename);"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:07.244998Z","iopub.status.busy":"2024-04-05T13:51:07.244649Z","iopub.status.idle":"2024-04-05T13:51:07.295527Z","shell.execute_reply":"2024-04-05T13:51:07.294609Z","shell.execute_reply.started":"2024-04-05T13:51:07.244971Z"},"trusted":true},"outputs":[],"source":["class seq2seq_with_attention(nn.Module):\n","    def __init__(self): #, encoder, decoder):\n","        super(seq2seq_with_attention, self).__init__()\n","        self.encoder = LSTM_on_words(200, 200, 2, encoder_vectors.wordvec, encoder_vectors.word_to_index['<PAD>'], bidirectional=True).to(device)\n","#         self.decoder = Decoder_LSTM(200, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n","        self.decoder = Decoder_LSTM_with_attention(600, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n","    \n","    def forward(self, problems, problem_lengths, linear_formulas = None):\n","        enc_out, (h_enc, c_enc) = self.encoder(problems, problem_lengths);\n","        hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n","        hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","        cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n","        cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","        \n","        outs, (h, c), attn = self.decoder(problems.shape[0],global_max_len, enc_out, hidden,cell, teacher_forcing = None if linear_formulas == None else linear_formulas.unsqueeze(-1)) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n","        return outs, (h,c), attn;\n","class seq2seq_normal(nn.Module):\n","    def __init__(self): #, encoder, decoder):\n","        super(seq2seq_normal, self).__init__()\n","        self.encoder = LSTM_on_words(200, 200, 2, encoder_vectors.wordvec, encoder_vectors.word_to_index['<PAD>'], bidirectional=True).to(device)\n","        self.decoder = Decoder_LSTM(200, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n","#         self.decoder = Decoder_LSTM_with_attention(600, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n","    def forward(self, problems, problem_lengths, linear_formulas = None):\n","        enc_out, (h_enc, c_enc) = self.encoder(problems, problem_lengths);\n","        hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n","        hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","        cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n","        cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","\n","        outs, (h, c), attn = self.decoder(problems.shape[0],global_max_len, enc_out, hidden,cell, teacher_forcing = None if linear_formulas == None else linear_formulas.unsqueeze(-1)) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n","        return outs, (h,c), attn;\n","class seq2seq_with_BERT(nn.Module):\n","    def __init__(self): #, encoder, decoder):\n","        super(seq2seq_with_BERT, self).__init__()\n","        self.encoder =  AutoModel.from_pretrained('bert-base-uncased');\n","        self.decoder = Decoder_LSTM(200, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n","    \n","model = seq2seq_with_attention(); \n","# model = seq2seq_normal() #with_attention();\n","optimizer = optim.Adam(model.parameters(), lr=0.001); \n","criterion = nn.CrossEntropyLoss(ignore_index=decoder_vectors.word_to_index['<PAD>']);\n","# criterion = nn.CrossEntropyLoss();\n","num_epochs = 20;"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:07.509080Z","iopub.status.busy":"2024-04-05T13:51:07.508304Z","iopub.status.idle":"2024-04-05T13:51:07.513101Z","shell.execute_reply":"2024-04-05T13:51:07.512222Z","shell.execute_reply.started":"2024-04-05T13:51:07.509051Z"},"trusted":true},"outputs":[],"source":["train_loss_curves = [];\n","dev_loss_curves = [];"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["model, optimizer, epoch, loss, encoder_vectors, decoder_vectors = load_checkpoint(model, optimizer, 'lstm_lstm_attn.pth');"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:07.893592Z","iopub.status.busy":"2024-04-05T13:51:07.893276Z","iopub.status.idle":"2024-04-05T13:51:07.903388Z","shell.execute_reply":"2024-04-05T13:51:07.902518Z","shell.execute_reply.started":"2024-04-05T13:51:07.893569Z"},"trusted":true},"outputs":[],"source":["def run_model(dataloader, model, training = True, verbose=0):\n","    running_sum = 0;\n","    total_batches = len(dataloader);\n","    done = 0;\n","    final_loss = 0;\n","    teacher_forcing = None;\n","    if(training):\n","        model.train();\n","    else:\n","        model.eval();\n","    for i, (problems, problem_lengths, linear_formulas) in enumerate(dataloader):\n","        problems = problems.to(device);\n","        # problems_lengths = torch.tensor(problems_lengths).to(device);\n","        linear_formulas = linear_formulas.to(device);\n","#         enc_out, (h_enc, c_enc) = model.encoder(problems, problem_lengths);\n","#         hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n","#         hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","#         cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n","#         cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n","# #         print(\"hidden:\",hidden[:,0,:200]) \n","# #         print(\"encout:\",enc_out[0,problem_lengths[0]-1])\n","#     #    hidden = h_enc[-1].unsqueeze(0); cell = c_enc[-1].unsqueeze(0);\n","# #         print(\"enc out:\", enc_out.shape, max(problem_lengths))\n","#         outs, (h, c), _ = model.decoder(problems.shape[0],global_max_len, enc_out, hidden,cell, teacher_forcing = linear_formulas.unsqueeze(-1)) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n","        outs, (h, c), _ = model(problems, problem_lengths, linear_formulas);\n","        #outs = decoder(batch_size,enc_out.shape[1], hidden, cell);\n","        #the first type of decoder does not have any attention system, so what it will do is simply take the last hidden state of the encoder and decipher it further based on that.\n","        loss = criterion(outs.view(-1, outs.shape[-1]), linear_formulas.view(-1));\n","        done += 1;\n","        running_sum += loss.item();\n","        if(training):\n","            optimizer.zero_grad();\n","            loss.backward();\n","            optimizer.step(); \n","        if(verbose > 0):  \n","            print(\"batch:\", i,\" out of\", total_batches,\" running loss:\",running_sum/done, end = \"                                   \\r\");\n","    final_loss = running_sum/total_batches; #this is the average loss.\n","    model.train(); #sets it back into training mode.\n","    return final_loss; "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:08.605043Z","iopub.status.busy":"2024-04-05T13:51:08.604234Z","iopub.status.idle":"2024-04-05T13:51:08.610531Z","shell.execute_reply":"2024-04-05T13:51:08.609610Z","shell.execute_reply.started":"2024-04-05T13:51:08.605013Z"},"trusted":true},"outputs":[],"source":["train_loss = [];\n","dev_loss = []; \n","def plot_losses(train_loss, dev_loss):\n","    fig, ax = plt.subplots(figsize=(10, 6))\n","    ax.plot(train_loss, label='Train')\n","    ax.plot(dev_loss, label='Dev')\n","    plt.title(\"Seq2Seq normal\")\n","    fig.legend()\n","    fig.savefig('seq2seq_normal.png')\n","    plt.show();"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:09.685577Z","iopub.status.busy":"2024-04-05T13:51:09.685207Z","iopub.status.idle":"2024-04-05T13:51:09.689724Z","shell.execute_reply":"2024-04-05T13:51:09.688782Z","shell.execute_reply.started":"2024-04-05T13:51:09.685548Z"},"trusted":true},"outputs":[],"source":["# load_checkpoint(model, optimizer, '/kaggle/working/lstm_lstm_attn.pth')"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:51:10.444296Z","iopub.status.busy":"2024-04-05T13:51:10.443564Z","iopub.status.idle":"2024-04-05T14:24:09.005338Z","shell.execute_reply":"2024-04-05T14:24:09.004410Z","shell.execute_reply.started":"2024-04-05T13:51:10.444266Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["batch: 92  out of 93  running loss: 0.6508481893488156                                     \r"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch:  0 avg loss: 0.8182965518006986 dev loss: 0.6508481893488156 Time taken: 310.07444024086\n","Epoch:  1 avg loss: 0.6070984121959698 dev loss: 0.6508481893488156 Time taken: 265.55709862709045\n","Epoch:  2 avg loss: 0.531528431109735 dev loss: 0.6508481893488156 Time taken: 334.3194365501404\n","batch: 39  out of 619  running loss: 0.4875949591398239                                    \r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m running_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[0;32m      5\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[1;32m----> 6\u001b[0m cur_epoch_loss \u001b[38;5;241m=\u001b[39m run_model(Train_loader, model, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m);\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      8\u001b[0m     cur_dev_loss \u001b[38;5;241m=\u001b[39m run_model(Dev_loader, model, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m); \u001b[38;5;66;03m#not training on the dev set.\u001b[39;00m\n","Cell \u001b[1;32mIn[49], line 33\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(dataloader, model, training, verbose)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(training):\n\u001b[0;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad();\n\u001b[1;32m---> 33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward();\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(); \n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):  \n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["total_batches = len(Train_loader);\n","for epoch in range(num_epochs):\n","    epoch_start = time.time();\n","    running_sum = 0;\n","    done = 0;\n","    cur_epoch_loss = run_model(Train_loader, model, training = True, verbose=1);\n","    if(epoch % 3 == 0):\n","        cur_dev_loss = run_model(Dev_loader, model, training = False, verbose=1); #not training on the dev set.\n","    train_loss.append(cur_epoch_loss);\n","    dev_loss.append(cur_dev_loss);\n","    if(epoch % 5 == 0):\n","        plot_losses(train_loss, dev_loss); #plotting our losses at each epoch.\n","    print(\"Epoch: \", epoch, \"avg loss:\", cur_epoch_loss, \"dev loss:\", cur_dev_loss, \"Time taken:\", time.time() - epoch_start);\n","    store_checkpoint(model, optimizer, epoch, cur_epoch_loss, 's9' + str(epoch) + '.pth', encoder_vectors, decoder_vectors);\n","    #now we will run it on the dev set to get loss curves.\n","    with open('9_tf_math_loss_curves.pkl', 'wb') as f:\n","        pickle.dump([train_loss, dev_loss], f);"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["def beam_search_output(problems, problem_lengths): #now we do beam search on this.\n","    #how to?\n","    #we need to keep track of the top k outputs at each step.\n","    k = 1;\n","    outs, (h, c), _ = model(problems, problem_lengths, None);\n","    return outs;\n","def get_sentence(indices):\n","    actual = indices;\n","    for i in range(len(indices)):\n","        if(indices[i] == decoder_vectors.word_to_index['<END>'] or indices[i] == decoder_vectors.word_to_index['<PAD>']):\n","            actual = indices[:i];\n","            break;\n","    return ''.join([decoder_vectors.index_to_word[i.item()] for i in actual]);"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["619"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["len(Train_loader)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Exact match: 2254 out of 3136 %: 0.71875                      \r"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.76 GiB is allocated by PyTorch, and 692.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[1;32mIn[79], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m linear_formulas \u001b[38;5;241m=\u001b[39m linear_formulas\u001b[38;5;241m.\u001b[39mto(device);\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# problems_lengths = torch.tensor(problems_lengths).to(device);\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outs \u001b[38;5;241m=\u001b[39m beam_search_output(problems, problem_lengths);\n\u001b[0;32m     10\u001b[0m decoder_vectors\u001b[38;5;241m.\u001b[39mword_to_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<END>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m outs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m);\n","Cell \u001b[1;32mIn[58], line 5\u001b[0m, in \u001b[0;36mbeam_search_output\u001b[1;34m(problems, problem_lengths)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbeam_search_output\u001b[39m(problems, problem_lengths): \u001b[38;5;66;03m#now we do beam search on this.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#how to?\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#we need to keep track of the top k outputs at each step.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m;\n\u001b[1;32m----> 5\u001b[0m     outs, (h, c), _ \u001b[38;5;241m=\u001b[39m model(problems, problem_lengths, \u001b[38;5;28;01mNone\u001b[39;00m);\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outs\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[75], line 15\u001b[0m, in \u001b[0;36mseq2seq_with_attention.forward\u001b[1;34m(self, problems, problem_lengths, linear_formulas)\u001b[0m\n\u001b[0;32m     12\u001b[0m cell \u001b[38;5;241m=\u001b[39m c_enc\u001b[38;5;241m.\u001b[39mview(c_enc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, c_enc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m];\n\u001b[0;32m     13\u001b[0m cell \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cell[\u001b[38;5;241m0\u001b[39m], cell[\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m); \u001b[38;5;66;03m#reverse and forward direction.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outs, (h, c), attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(problems\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],global_max_len, enc_out, hidden,cell, teacher_forcing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m linear_formulas \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m linear_formulas\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#, teacher_forcing = linear_formulas.unsqueeze(-1));\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, (h,c), attn\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[68], line 83\u001b[0m, in \u001b[0;36mDecoder_LSTM_with_attention.forward\u001b[1;34m(self, batch_size, max_len, encoder_outputs, hidden, cell, teacher_forcing)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# print(\"teacher forcing:\", teacher_forcing.shape)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[1;32m---> 83\u001b[0m     dec_out, (hidden, cell), attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_step(dec_in,hidden, cell, encoder_outputs); \u001b[38;5;66;03m#we get the value after one step of the LSTM.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(dec_out);\n\u001b[0;32m     85\u001b[0m     attentions\u001b[38;5;241m.\u001b[39mappend(attention_weights);\n","Cell \u001b[1;32mIn[68], line 103\u001b[0m, in \u001b[0;36mDecoder_LSTM_with_attention.forward_step\u001b[1;34m(self, inputs, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(inputs);\n\u001b[0;32m    102\u001b[0m         query \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m);\n\u001b[1;32m--> 103\u001b[0m         context, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(query, encoder_outputs);\n\u001b[0;32m    104\u001b[0m         inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((outs, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m); \u001b[38;5;66;03m#concatenating them.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m#         print(\"input is: \", outs.shape, \" + \", context.shape, \" = \", outs.shape[2] + context.shape[2]);\u001b[39;00m\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[68], line 54\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden, encoder_outputs): \n\u001b[1;32m---> 54\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mValueMatrix(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQueryMatrix(hidden) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKeyMatrix(encoder_outputs)));\n\u001b[0;32m     55\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m);\n\u001b[0;32m     56\u001b[0m     weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m);\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.76 GiB is allocated by PyTorch, and 692.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["exacts = 0;\n","total = 0;\n","# dataloader = DataLoader(mathDataset(val_data, global_max_len), batch_size=1, shuffle=True, collate_fn=collate_fn);\n","\n","for i, (problems, problem_lengths, linear_formulas) in enumerate(Train_loader):\n","    problems = problems.to(device);\n","    linear_formulas = linear_formulas.to(device);\n","    # problems_lengths = torch.tensor(problems_lengths).to(device);\n","    outs = beam_search_output(problems, problem_lengths);\n","    decoder_vectors.word_to_index['<END>']\n","    preds = outs.argmax(dim=-1);\n","    predicted_sentence = [get_sentence(preds[i]) for i in range(preds.shape[0])];\n","    actual_sentence = [get_sentence(linear_formulas[i]) for i in range(linear_formulas.shape[0])];\n","    match = [predicted_sentence[i] == actual_sentence[i] for i in range(len(predicted_sentence))];\n","    exacts += sum(match);\n","    total += len(match);\n","    print(\"Exact match:\", exacts, \"out of\", total, \"%:\", exacts/total, end = \"           \\r\");\n","    # for i in range(len(indices)):\n","    #     test_data[indices[i]]['predicted'] = predicted_sentence[i];"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T14:33:42.701472Z","iopub.status.busy":"2024-04-05T14:33:42.701093Z","iopub.status.idle":"2024-04-05T14:33:42.837610Z","shell.execute_reply":"2024-04-05T14:33:42.836678Z","shell.execute_reply.started":"2024-04-05T14:33:42.701445Z"},"trusted":true},"outputs":[],"source":["#now we will save the lists that we have prepared.\n","store_checkpoint(model, optimizer, epoch, cur_epoch_loss, 'lstm_lstm_attn.pth', encoder_vectors, decoder_vectors);"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4734131,"sourceId":8031681,"sourceType":"datasetVersion"},{"datasetId":4737721,"sourceId":8036634,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
