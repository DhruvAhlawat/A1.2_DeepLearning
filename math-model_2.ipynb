{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7984457,"sourceType":"datasetVersion","datasetId":4699864}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport json\nimport torch.nn as nn\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport gensim #For word2vec\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nimport time\nimport nltk\nimport random\nfrom numpy.random import choice as randomchoice\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport sys\n# import torchtext\nimport pickle\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport matplotlib.pyplot as plt\nimport matplotlib as mlt","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:26:29.077937Z","iopub.execute_input":"2024-04-01T22:26:29.078875Z","iopub.status.idle":"2024-04-01T22:26:29.086557Z","shell.execute_reply.started":"2024-04-01T22:26:29.078843Z","shell.execute_reply":"2024-04-01T22:26:29.085488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim.downloader as api\nimport torchtext.vocab as vocab\nglove_model = api.load(\"glove-wiki-gigaword-200\")","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:26:29.897600Z","iopub.execute_input":"2024-04-01T22:26:29.898524Z","iopub.status.idle":"2024-04-01T22:27:49.977450Z","shell.execute_reply.started":"2024-04-01T22:26:29.898493Z","shell.execute_reply":"2024-04-01T22:27:49.976214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_start_time = time.time();\n# val_file = sys.argv[2];\n# train_file = 'data/train.json'\n# val_file = 'data/dev.json'\n# test_file = 'data/test.json'\ntrain_file = '/kaggle/input/a1-part2-data/data/train.json'\nval_file = '/kaggle/input/a1-part2-data/data/dev.json'\ntest_file = '/kaggle/input/a1-part2-data/data/dev.json'\n\ntokenize_func = nltk.tokenize.WordPunctTokenizer().tokenize\npunctuations = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'\ndef is_numeric(s):\n    try:\n        float(s)\n        return True\n    except ValueError: #Classic way to get is_numeric\n        return False\ndef tokenize(sentence, with_num = False):\n    old = tokenize_func(sentence.lower());\n    s = [];\n    for word in old:\n        running = [];\n        for character in word:\n            if(character in punctuations):\n                if(len(running) > 0):\n                    s.append(''.join(running));\n                    running = []; #emptying the running list.\n                s.append(character); #then adding the punctuation.\n            else:\n                running.append(character);\n        if(len(running) > 0):\n            s.append(''.join(running));\n        #this above code ensures that what we have is also split on punctuation\n    if(with_num):\n        return s; #If with_num is true, return the sentence as it is, without converting the numbers to <NUM>\n    for i in range(len(s)):\n        if(is_numeric(s[i])):\n            s[i] = '<NUM>'; #replaces numbers with <NUM>\n    return s;\n\ndef tokenize_with_num(sentence): #just tokenizes normally. No replacement of numbers\n    s = tokenize_func(sentence.lower());\n    return s;\n\ndef get_embedding_index(sentences, model):\n    return ([tokenize_and_get_embedding_index(sentence, model) for sentence in sentences]);\n\n\ndef tokenize_and_get_embedding_index_as_list(sentence, vocab, with_num = False):\n    s = tokenize(sentence, with_num = with_num);\n    # FOr now testing with No UNK, Later will have to add UNK\n    tens = ([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n    return tens;\n\ndef tokenize_and_get_embedding_index(sentence, vocab, with_num = False):\n    s = tokenize(sentence, with_num = with_num);\n    # FOr now testing with No UNK, Later will have to add UNK\n    tens = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n    return tens;\n    if(len(tens) == 0):\n        return torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) #using UNK in this case.\n    else:\n        return tens;","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:05.044758Z","iopub.execute_input":"2024-04-01T22:35:05.045155Z","iopub.status.idle":"2024-04-01T22:35:05.059739Z","shell.execute_reply.started":"2024-04-01T22:35:05.045127Z","shell.execute_reply":"2024-04-01T22:35:05.058655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(train_file) as f:\n    train_data = json.load(f)\nwith open(val_file) as f:\n    val_data = json.load(f)\nwith open(test_file) as f:\n    test_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:05.921110Z","iopub.execute_input":"2024-04-01T22:35:05.921466Z","iopub.status.idle":"2024-04-01T22:35:06.016886Z","shell.execute_reply.started":"2024-04-01T22:35:05.921438Z","shell.execute_reply":"2024-04-01T22:35:06.016095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_max_len = 120;","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:06.634972Z","iopub.execute_input":"2024-04-01T22:35:06.635339Z","iopub.status.idle":"2024-04-01T22:35:06.639628Z","shell.execute_reply.started":"2024-04-01T22:35:06.635311Z","shell.execute_reply":"2024-04-01T22:35:06.638634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Postprocessing the code to remove the last '|' that is sometimes randomly present.\ndef remove_last_extra(data):\n    for i in range(len(data)):\n        if(data[i]['linear_formula'][-1] == '|'):\n            data[i]['linear_formula'] = data[i]['linear_formula'][:-1];\n        \n    return data; #although not really needed.\nremove_last_extra(val_data);\nremove_last_extra(train_data);\nremove_last_extra(test_data);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:07.146981Z","iopub.execute_input":"2024-04-01T22:35:07.147782Z","iopub.status.idle":"2024-04-01T22:35:07.168701Z","shell.execute_reply.started":"2024-04-01T22:35:07.147750Z","shell.execute_reply":"2024-04-01T22:35:07.167627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class glove_vectors():\n    def get_word_embedding(word, glove_vectors, dim):\n        if word in glove_vectors.key_to_index: #if the key is present we initialize it as glove embedding\n            return torch.tensor(glove_vectors[word])\n        else:\n            return torch.rand(dim)  # Initi\n    def __init__(self, sentences, glove_model, dim=200):\n        self.vocabulary = set(['<START>', '<END>', '<PAD>', '<UNK>', '<NUM>']);\n        for sentence in sentences:\n            for word in tokenize(sentence):\n                self.vocabulary.add(word); #creates the vocabulary.\n        self.word_to_index = {word: i for i, word in enumerate(self.vocabulary)};\n        self.index_to_word = {i: word for i, word in enumerate(self.vocabulary)};\n        self.wordvec = [0] * len(self.vocabulary); #initializing the encoder_wordvec list\n        rand_count = 0;\n        for i in range(len(self.vocabulary)):\n            self.wordvec[i] = glove_vectors.get_word_embedding(self.index_to_word[i], glove_model, dim);\n        self.wordvec = torch.stack(self.wordvec); #stacking the list of tensors to form a tensor.","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:07.804217Z","iopub.execute_input":"2024-04-01T22:35:07.804620Z","iopub.status.idle":"2024-04-01T22:35:07.813422Z","shell.execute_reply.started":"2024-04-01T22:35:07.804592Z","shell.execute_reply":"2024-04-01T22:35:07.812435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_vectors = glove_vectors([data['Problem'] for data in train_data], glove_model);\ndecoder_vectors = glove_vectors([data['linear_formula'] for data in train_data], glove_model);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:08.477852Z","iopub.execute_input":"2024-04-01T22:35:08.478568Z","iopub.status.idle":"2024-04-01T22:35:12.205681Z","shell.execute_reply.started":"2024-04-01T22:35:08.478528Z","shell.execute_reply":"2024-04-01T22:35:12.204849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('mysaved.pkl', 'wb') as f:\n    pickle.dump([encoder_vectors, decoder_vectors], f);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T22:35:12.207246Z","iopub.execute_input":"2024-04-01T22:35:12.207693Z","iopub.status.idle":"2024-04-01T22:35:12.227002Z","shell.execute_reply.started":"2024-04-01T22:35:12.207640Z","shell.execute_reply":"2024-04-01T22:35:12.226111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM_on_words(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index,bidirectional=True, dropout=0.0):\n        super(LSTM_on_words, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=False).to(device);\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional).to(device);\n\n    def forward(self, x, x_lengths):\n        # Embedding\n        out = self.embedding(x)\n        # Pack padded sequence\n        # lengths = x_lengths.detach().cpu().numpy();\n        out = pack_padded_sequence(out, x_lengths, batch_first=True, enforce_sorted=False).to(device);\n        out, (hidden, cell) = self.lstm(out)\n        # Unpack packed sequence\n        out, _ = pad_packed_sequence(out, batch_first=True)\n        return out, (hidden, cell);\n\nclass FeedForward(nn.Module):\n    def __init__(self, input_size, layer_sizes):\n        super(FeedForward, self).__init__()\n        self.layers = [];\n        self.ReLU = nn.ReLU(inplace=False)\n        for i in range(len(layer_sizes)):\n            if(i == 0):\n                self.layers.append(nn.Linear(input_size, layer_sizes[i]));\n            else:\n                self.layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]));\n            if(i != len(layer_sizes) - 1): #add Relu only if its not the last layer, since that is the output layer that we will softmax over.\n                self.layers.append(self.ReLU);\n        self.all_layers = nn.Sequential(*self.layers)\n    def forward(self, x):\n        out = self.all_layers(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:25.252205Z","iopub.status.idle":"2024-04-01T23:16:25.252601Z","shell.execute_reply.started":"2024-04-01T23:16:25.252418Z","shell.execute_reply":"2024-04-01T23:16:25.252432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class mathDataset(Dataset):\n    def __init__(self, data, global_max_len = global_max_len):\n        self.data = data;\n        # self.encoder_word_to_index = encoder_wordvec.word_to_index;\n        # self.vocab_index_to_word = vocab_index_to_word;\n        self.max_len = global_max_len;\n    def __len__(self):\n        return len(self.data);\n    def __getitem__(self, idx):\n        problem = self.data[idx]['Problem'];\n        linear_formula = self.data[idx]['linear_formula']; #maybe the linear formula can go directly without getting emebdded as well.\n        problem = tokenize_and_get_embedding_index_as_list(problem, encoder_vectors.word_to_index);\n        problem.append(encoder_vectors.word_to_index['<END>']);\n        problem = torch.tensor(problem);\n        linear_formula = tokenize_and_get_embedding_index_as_list(linear_formula, decoder_vectors.word_to_index);\n        linear_formula.append(decoder_vectors.word_to_index['<END>']);\n        #we need this linear formula to be of a constant size.\n        padding_len = self.max_len - len(linear_formula)\n        linear_formula = linear_formula[:self.max_len];\n        if padding_len > 0:\n            linear_formula += [decoder_vectors.word_to_index['<PAD>']] * padding_len\n        linear_formula = torch.tensor(linear_formula)\n        return problem, linear_formula;\n\ndef collate_fn(data):\n    # data.sort(key=lambda x: len(x[0]), reverse=True)\n    problems, linear_formulas = zip(*data)\n    # problems = data; #zip(*data)\n    problems_lengths = [len(problem) for problem in problems]\n    # linear_formulas = pad_sequence\n    problems = pad_sequence(problems, batch_first=True, padding_value=encoder_vectors.word_to_index['<PAD>'])\n    linear_formulas = pad_sequence(linear_formulas, batch_first=True, padding_value=decoder_vectors.word_to_index['<PAD>'])\n    return problems, problems_lengths, linear_formulas;\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:25.254143Z","iopub.status.idle":"2024-04-01T23:16:25.254478Z","shell.execute_reply.started":"2024-04-01T23:16:25.254315Z","shell.execute_reply":"2024-04-01T23:16:25.254329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = mathDataset(train_data, global_max_len)\nbatch_size = min(32, len(train_dataset));\nTrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn);\nDev_loader = DataLoader(mathDataset(val_data, global_max_len), batch_size=batch_size, shuffle=True, collate_fn=collate_fn);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:25.256077Z","iopub.status.idle":"2024-04-01T23:16:25.256431Z","shell.execute_reply.started":"2024-04-01T23:16:25.256264Z","shell.execute_reply":"2024-04-01T23:16:25.256279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#so our encoder is simply LSTM_on_words. Now to make the decoder LSTM_on_words.\nteacher_forcing_probability = 0.6;\nclass Decoder_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n        super(Decoder_LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=False).to(device);\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n        self.fc = FeedForward(hidden_size,[len(wordvectors)*2,len(wordvectors)]).to(device);\n#         self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n    def forward(self, batch_size,max_len, hidden, cell, teacher_forcing = None):\n        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n        outputs = [];\n        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n        # print(\"teacher forcing:\", teacher_forcing.shape)\n        for i in range(max_len):\n            dec_out, (hidden, cell) = self.forward_step(dec_in,hidden, cell); #we get the value after one step of the LSTM.\n            outputs.append(dec_out);\n            # print(dec_out.shape);\n            if(teacher_forcing == None):\n                _, ind = dec_out.topk(1);\n                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n            else:\n                if(random.random() > teacher_forcing_probability):\n                    _, ind = dec_out.topk(1);\n                    dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n                else:\n                    dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n        outputs = torch.cat(outputs, dim=1)\n        return outputs, (hidden, cell)\n    \n    def forward_step(self, inputs, hidden, cell):\n        outs = self.embedding(inputs);\n        outs, (h, c)  = self.lstm(outs, (hidden, cell));\n        outs = self.fc(outs); #\n        return outs, (h, c);","metadata":{"execution":{"iopub.status.busy":"2024-04-02T07:26:56.744214Z","iopub.execute_input":"2024-04-02T07:26:56.744542Z","iopub.status.idle":"2024-04-02T07:26:57.104303Z","shell.execute_reply.started":"2024-04-02T07:26:56.744516Z","shell.execute_reply":"2024-04-02T07:26:57.103074Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#so our encoder is simply LSTM_on_words. Now to make the decoder LSTM_on_words.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m teacher_forcing_probability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m;\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecoder_LSTM\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Decoder_LSTM, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"],"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, filename):\n    checkpoint = torch.load(filename);\n    model.load_state_dict(checkpoint['model_state_dict']);\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict']);\n    epoch = checkpoint['epoch'];\n    loss = checkpoint['loss'];\n    return model, optimizer, epoch, loss;\n\ndef store_checkpoint(model, optimizer, epoch, loss, filename):\n    torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'epoch': epoch,\n            'loss': loss,\n            }, filename);\ndef plot_losses(train_loss, dev_loss):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(train_loss, label='Train')\n    ax.plot(dev_loss, label='Dev')\n    fig.legend();\n    fig.savefig('math_loss_curves.png')","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:59.998662Z","iopub.execute_input":"2024-04-01T23:16:59.999048Z","iopub.status.idle":"2024-04-01T23:17:00.006073Z","shell.execute_reply.started":"2024-04-01T23:16:59.999023Z","shell.execute_reply":"2024-04-01T23:17:00.005026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class seq2seq(nn.Module):\n    def __init__(self): #, encoder, decoder):\n        super(seq2seq, self).__init__()\n        self.encoder = LSTM_on_words(200, 200, 2, encoder_vectors.wordvec, encoder_vectors.word_to_index['<PAD>'], bidirectional=True).to(device)\n        self.decoder = Decoder_LSTM(200, 400, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n    \n\nmodel = seq2seq(); \noptimizer = optim.Adam(model.parameters(), lr=0.001); \ncriterion = nn.CrossEntropyLoss(ignore_index=decoder_vectors.word_to_index['<PAD>']);\n# criterion = nn.CrossEntropyLoss();\nnum_epochs = 50;","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:09.931866Z","iopub.execute_input":"2024-04-01T23:17:09.932611Z","iopub.status.idle":"2024-04-01T23:17:09.969842Z","shell.execute_reply.started":"2024-04-01T23:17:09.932571Z","shell.execute_reply":"2024-04-01T23:17:09.968863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_curves = [];\ndev_loss_curves = [];","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:11.189185Z","iopub.execute_input":"2024-04-01T23:17:11.190132Z","iopub.status.idle":"2024-04-01T23:17:11.194224Z","shell.execute_reply.started":"2024-04-01T23:17:11.190090Z","shell.execute_reply":"2024-04-01T23:17:11.193233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_model(dataloader, model, training = True, verbose=0):\n    running_sum = 0;\n    total_batches = len(dataloader);\n    done = 0;\n    final_loss = 0;\n    if(training):\n        model.train();\n    else:\n        model.eval();\n    for i, (problems, problems_lengths, linear_formulas) in enumerate(dataloader):\n        problems = problems.to(device);\n        # problems_lengths = torch.tensor(problems_lengths).to(device);\n        linear_formulas = linear_formulas.to(device);\n        enc_out, (h_enc, c_enc) = model.encoder(problems, problems_lengths);\n        hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n        hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n        cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n        cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n#         hidden = h_enc[-1].unsqueeze(0); cell = c_enc[-1].unsqueeze(0);\n        outs, (h, c) = model.decoder(problems.shape[0],global_max_len,hidden,cell, teacher_forcing = linear_formulas.unsqueeze(-1)) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n        #outs = decoder(batch_size,enc_out.shape[1], hidden, cell);\n        #the first type of decoder does not have any attention system, so what it will do is simply take the last hidden state of the encoder and decipher it further based on that.\n        loss = criterion(outs.view(-1, outs.shape[-1]), linear_formulas.view(-1));\n        done += 1;\n        running_sum += loss.item();\n        if(training):\n            optimizer.zero_grad();\n            loss.backward();\n            optimizer.step(); \n        if(verbose > 0):\n            print(\"batch:\", i,\" out of\", total_batches,\" running loss:\",running_sum/done, end = \"                                   \\r\");\n    final_loss = running_sum/total_batches; #this is the average loss.\n    return final_loss;\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:12.295453Z","iopub.execute_input":"2024-04-01T23:17:12.295845Z","iopub.status.idle":"2024-04-01T23:17:12.308034Z","shell.execute_reply.started":"2024-04-01T23:17:12.295814Z","shell.execute_reply":"2024-04-01T23:17:12.307040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir data\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:13.727629Z","iopub.execute_input":"2024-04-01T23:17:13.728380Z","iopub.status.idle":"2024-04-01T23:17:14.761938Z","shell.execute_reply.started":"2024-04-01T23:17:13.728345Z","shell.execute_reply":"2024-04-01T23:17:14.760741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = [];\ndev_loss = [];","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:14.763905Z","iopub.execute_input":"2024-04-01T23:17:14.764184Z","iopub.status.idle":"2024-04-01T23:17:14.769415Z","shell.execute_reply.started":"2024-04-01T23:17:14.764156Z","shell.execute_reply":"2024-04-01T23:17:14.768413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_batches = len(Train_loader);\n\nfor epoch in range(num_epochs):\n    epoch_start = time.time();\n    running_sum = 0;\n    done = 0;\n    cur_epoch_loss = run_model(Train_loader, model, training = True, verbose=0);\n    cur_dev_loss = run_model(Dev_loader, model, training = False, verbose=0); #not training on the dev set.\n    train_loss.append(cur_epoch_loss);\n    dev_loss.append(cur_dev_loss);\n    print(\"Epoch: \", epoch, \"train loss:\", cur_epoch_loss, \"dev loss:\", cur_dev_loss, \"Time taken:\", time.time() - epoch_start);\n    store_checkpoint(model, optimizer, epoch, cur_epoch_loss, 'checkpoint' + str(epoch) + '.pth');\n    with open('math_loss_curves.pkl', 'wb') as f:\n        pickle.dump([train_loss, dev_loss], f);\n    plot_losses(train_loss, dev_loss);\n    #now we will run it on the dev set to get loss curves.","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:17:16.280733Z","iopub.execute_input":"2024-04-01T23:17:16.281355Z","iopub.status.idle":"2024-04-01T23:31:34.873572Z","shell.execute_reply.started":"2024-04-01T23:17:16.281324Z","shell.execute_reply":"2024-04-01T23:31:34.872092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we will save the lists that we have prepared.\nwith open('math_loss_curves.pkl', 'wb') as f:\n    pickle.dump([train_loss, dev_loss], f);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:29.688064Z","iopub.execute_input":"2024-04-01T23:16:29.688425Z","iopub.status.idle":"2024-04-01T23:16:29.693489Z","shell.execute_reply.started":"2024-04-01T23:16:29.688398Z","shell.execute_reply":"2024-04-01T23:16:29.692599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplot_losses(train_loss, dev_loss);","metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:16:30.507590Z","iopub.execute_input":"2024-04-01T23:16:30.508258Z","iopub.status.idle":"2024-04-01T23:16:30.854550Z","shell.execute_reply.started":"2024-04-01T23:16:30.508223Z","shell.execute_reply":"2024-04-01T23:16:30.853621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}