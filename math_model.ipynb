{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gensim #For word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import nltk\n",
    "import random\n",
    "from numpy.random import choice as randomchoice\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import sys\n",
    "# import torchtext\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import torchtext.vocab as vocab\n",
    "glove_model = api.load(\"glove-wiki-gigaword-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start_time = time.time();\n",
    "# val_file = sys.argv[2];\n",
    "train_file = 'data/train.json'\n",
    "val_file = 'data/dev.json'\n",
    "test_file = 'data/test.json'\n",
    "\n",
    "tokenize_func = nltk.tokenize.WordPunctTokenizer().tokenize\n",
    "punctuations = '!\"#$%&\\'()*+,-./:;=?@[\\\\]^_`{|}~'\n",
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError: #Classic way to get is_numeric\n",
    "        return False\n",
    "def tokenize(sentence, with_num = False):\n",
    "    old = tokenize_func(sentence.lower());\n",
    "    s = [];\n",
    "    for word in old:\n",
    "        running = [];\n",
    "        for character in word:\n",
    "            if(character in punctuations):\n",
    "                if(len(running) > 0):\n",
    "                    s.append(''.join(running));\n",
    "                    running = []; #emptying the running list.\n",
    "                s.append(character); #then adding the punctuation.\n",
    "            else:\n",
    "                running.append(character);\n",
    "        if(len(running) > 0):\n",
    "            s.append(''.join(running));\n",
    "        #this above code ensures that what we have is also split on punctuation\n",
    "    if(with_num):\n",
    "        return s; #If with_num is true, return the sentence as it is, without converting the numbers to <NUM>\n",
    "    for i in range(len(s)):\n",
    "        if(is_numeric(s[i])):\n",
    "            s[i] = '<NUM>'; #replaces numbers with <NUM>\n",
    "    return s;\n",
    "\n",
    "def tokenize_with_num(sentence): #just tokenizes normally. No replacement of numbers\n",
    "    s = tokenize_func(sentence.lower());\n",
    "    return s;\n",
    "\n",
    "def get_embedding_index(sentences, model):\n",
    "    return ([tokenize_and_get_embedding_index(sentence, model) for sentence in sentences]);\n",
    "\n",
    "\n",
    "def tokenize_and_get_embedding_index_as_list(sentence, vocab, with_num = False):\n",
    "    s = tokenize(sentence, with_num = with_num);\n",
    "    # FOr now testing with No UNK, Later will have to add UNK\n",
    "    tens = ([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n",
    "    return tens;\n",
    "\n",
    "def tokenize_and_get_embedding_index(sentence, vocab, with_num = False):\n",
    "    s = tokenize(sentence, with_num = with_num);\n",
    "    # FOr now testing with No UNK, Later will have to add UNK\n",
    "    tens = torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) # if (word in vocab)]); #if the word is not in the punctuation, only then we add it.\n",
    "    return tens;\n",
    "    if(len(tens) == 0):\n",
    "        return torch.tensor([vocab.get(word, vocab['<UNK>']) for word in s]) #using UNK in this case.\n",
    "    else:\n",
    "        return tens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '<', 'end', '>']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"hello world <END>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(val_file) as f:\n",
    "    val_data = json.load(f)\n",
    "with open(test_file) as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sophia finished 2 / 3 of a book . she calculated that she finished 90 more pages than she has yet to read . how long is her book ?'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]['Problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_len = 120;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Postprocessing the code to remove the last '|' that is sometimes randomly present.\n",
    "def remove_last_extra(data):\n",
    "    for i in range(len(data)):\n",
    "        if(data[i]['linear_formula'][-1] == '|'):\n",
    "            data[i]['linear_formula'] = data[i]['linear_formula'][:-1];\n",
    "        \n",
    "    return data; #although not really needed.\n",
    "remove_last_extra(val_data);\n",
    "remove_last_extra(train_data);\n",
    "remove_last_extra(test_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glove_vectors():\n",
    "    def get_word_embedding(word, glove_vectors, dim):\n",
    "        if word in glove_vectors.key_to_index: #if the key is present we initialize it as glove embedding\n",
    "            return torch.tensor(glove_vectors[word])\n",
    "        else:\n",
    "            return torch.rand(dim)  # Initi\n",
    "    def __init__(self, sentences, glove_model, dim=200):\n",
    "        self.vocabulary = set(['<START>', '<END>', '<PAD>', '<UNK>', '<NUM>']);\n",
    "        for sentence in sentences:\n",
    "            for word in tokenize(sentence):\n",
    "                self.vocabulary.add(word); #creates the vocabulary.\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.vocabulary)};\n",
    "        self.index_to_word = {i: word for i, word in enumerate(self.vocabulary)};\n",
    "        self.wordvec = [0] * len(self.vocabulary); #initializing the encoder_wordvec list\n",
    "        rand_count = 0;\n",
    "        for i in range(len(self.vocabulary)):\n",
    "            self.wordvec[i] = glove_vectors.get_word_embedding(self.index_to_word[i], glove_model, dim);\n",
    "        self.wordvec = torch.stack(self.wordvec); #stacking the list of tensors to form a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vectors = glove_vectors([data['Problem'] for data in train_data], glove_model);\n",
    "decoder_vectors = glove_vectors([data['linear_formula'] for data in train_data], glove_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mysaved.pkl', 'wb') as f:\n",
    "    pickle.dump([encoder_vectors, decoder_vectors], f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_on_words(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index,bidirectional=True, dropout=0.0):\n",
    "        super(LSTM_on_words, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional).to(device);\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        # Embedding\n",
    "        out = self.embedding(x)\n",
    "        # Pack padded sequence\n",
    "        # lengths = x_lengths.detach().cpu().numpy();\n",
    "        out = pack_padded_sequence(out, x_lengths, batch_first=True, enforce_sorted=False).to(device);\n",
    "        out, (hidden, cell) = self.lstm(out)\n",
    "        # Unpack packed sequence\n",
    "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        return out, (hidden, cell);\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layers = [];\n",
    "        self.ReLU = nn.ReLU(inplace=False)\n",
    "        for i in range(len(layer_sizes)):\n",
    "            if(i == 0):\n",
    "                self.layers.append(nn.Linear(input_size, layer_sizes[i]));\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_sizes[i-1], layer_sizes[i]));\n",
    "            if(i != len(layer_sizes) - 1): #add Relu only if its not the last layer, since that is the output layer that we will softmax over.\n",
    "                self.layers.append(self.ReLU);\n",
    "        self.all_layers = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        out = self.all_layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mathDataset(Dataset):\n",
    "    def __init__(self, data, global_max_len = global_max_len):\n",
    "        self.data = data;\n",
    "        # self.encoder_word_to_index = encoder_wordvec.word_to_index;\n",
    "        # self.vocab_index_to_word = vocab_index_to_word;\n",
    "        self.max_len = global_max_len;\n",
    "    def __len__(self):\n",
    "        return 2;\n",
    "        return len(self.data);\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.data[idx]['Problem'];\n",
    "        linear_formula = self.data[idx]['linear_formula']; #maybe the linear formula can go directly without getting emebdded as well.\n",
    "        problem = tokenize_and_get_embedding_index_as_list(problem, encoder_vectors.word_to_index);\n",
    "        problem.append(encoder_vectors.word_to_index['<END>']);\n",
    "        problem = torch.tensor(problem);\n",
    "        linear_formula = tokenize_and_get_embedding_index_as_list(linear_formula, decoder_vectors.word_to_index);\n",
    "        linear_formula.append(decoder_vectors.word_to_index['<END>']);\n",
    "        #we need this linear formula to be of a constant size.\n",
    "        padding_len = self.max_len - len(linear_formula)\n",
    "        linear_formula = linear_formula[:self.max_len];\n",
    "        if padding_len > 0:\n",
    "            linear_formula += [decoder_vectors.word_to_index['<PAD>']] * padding_len\n",
    "        linear_formula = torch.tensor(linear_formula)\n",
    "        return problem, linear_formula;\n",
    "\n",
    "def collate_fn(data):\n",
    "    # data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    problems, linear_formulas = zip(*data)\n",
    "    # problems = data; #zip(*data)\n",
    "    problems_lengths = [len(problem) for problem in problems]\n",
    "    # linear_formulas = pad_sequence\n",
    "    problems = pad_sequence(problems, batch_first=True, padding_value=encoder_vectors.word_to_index['<PAD>'])\n",
    "    linear_formulas = pad_sequence(linear_formulas, batch_first=True, padding_value=decoder_vectors.word_to_index['<PAD>'])\n",
    "    return problems, problems_lengths, linear_formulas;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mathDataset(train_data, global_max_len)\n",
    "batch_size = min(32, len(train_dataset));\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so our encoder is simply LSTM_on_words. Now to make the decoder LSTM_on_words.\n",
    "class Decoder_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, wordvectors, padding_index, dropout=0.0):\n",
    "        super(Decoder_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(wordvectors), padding_idx=padding_index,freeze=True).to(device);\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True).to(device);\n",
    "        # self.fc = FeedForward(hidden_size,[len(wordvectors)//2,len(wordvectors)//2,len(wordvectors)]).to(device);\n",
    "        self.fc = nn.Linear(hidden_size, len(wordvectors)).to(device);\n",
    "    def forward(self, batch_size,max_len, hidden, cell, teacher_forcing = None):\n",
    "        # dec_in = torch.tensor([decoder_vectors.word_to_index['<START>']] * batch_size).unsqueeze(1).to(device);\n",
    "        dec_in = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(decoder_vectors.word_to_index['<START>']);\n",
    "        outputs = [];\n",
    "        # print(\"hidden:\", hidden.shape, \"cell:\", cell.shape, \"dec_in:\", dec_in.shape)\n",
    "        # print(\"teacher forcing:\", teacher_forcing.shape)\n",
    "        for i in range(max_len):\n",
    "            dec_out, (hidden, cell) = self.forward_step(dec_in,hidden, cell); #we get the value after one step of the LSTM.\n",
    "            outputs.append(dec_out);\n",
    "            # print(dec_out.shape);\n",
    "            if(teacher_forcing == None):\n",
    "                _, ind = dec_out.topk(1);\n",
    "                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "            else:\n",
    "                _, ind = dec_out.topk(1);\n",
    "                dec_in = ind.squeeze(-1).detach(); #squeezing is necessary because there will be an extra dimension here. Detaching it from the next step.\n",
    "                # print(\"output is \", dec_in, \"but teacher forced to use: \", teacher_forcing[:,i]);\n",
    "                dec_in = teacher_forcing[:,i]; #at the ith position of all the batches we have what we need.\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, (hidden, cell)\n",
    "    \n",
    "    def forward_step(self, inputs, hidden, cell):\n",
    "        outs = self.embedding(inputs);\n",
    "        outs, (h, c)  = self.lstm(outs, (hidden, cell));\n",
    "        outs = self.fc(outs); #\n",
    "        return outs, (h, c);\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self): #, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = LSTM_on_words(200, 200, 2, encoder_vectors.wordvec, encoder_vectors.word_to_index['<PAD>'], bidirectional=False).to(device)\n",
    "        self.decoder = Decoder_LSTM(200, 200, 1, decoder_vectors.wordvec, decoder_vectors.word_to_index['<PAD>']).to(device)\n",
    "    \n",
    "\n",
    "model = seq2seq(); \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001); \n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=vocab_word_to_index['<PAD>']);\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "num_epochs = 200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time();\n",
    "    running_sum = 0;\n",
    "    done = 0;\n",
    "    for i, (problems, problems_lengths, linear_formulas) in enumerate(dataloader):\n",
    "        problems = problems.to(device);\n",
    "        # problems_lengths = torch.tensor(problems_lengths).to(device);\n",
    "        linear_formulas = linear_formulas.to(device);\n",
    "        enc_out, (h_enc, c_enc) = model.encoder(problems, problems_lengths);\n",
    "#         hidden = h_enc.view(h_enc.shape[0]//2, 2, h_enc.shape[1], -1)[-1];\n",
    "#         hidden = torch.cat((hidden[0], hidden[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n",
    "#         cell = c_enc.view(c_enc.shape[0]//2, 2, c_enc.shape[1], -1)[-1];\n",
    "#         cell = torch.cat((cell[0], cell[1]), dim=-1).unsqueeze(0); #reverse and forward direction.\n",
    "        hidden = h_enc[-1].unsqueeze(0); cell = c_enc[-1].unsqueeze(0);\n",
    "        outs, (h, c) = model.decoder(batch_size,global_max_len,hidden,cell) #, teacher_forcing = linear_formulas.unsqueeze(-1));\n",
    "        #outs = decoder(batch_size,enc_out.shape[1], hidden, cell);\n",
    "        #the first type of decoder does not have any attention system, so what it will do is simply take the last hidden state of the encoder and decipher it further based on that.\n",
    "        optimizer.zero_grad();\n",
    "        loss = criterion(outs.view(-1, outs.shape[-1]), linear_formulas.view(-1));\n",
    "        loss.backward();\n",
    "        optimizer.step(); \n",
    "        # print(\"loss:\",loss.item(), end = \"                                   \\r\");\n",
    "        running_sum += loss.item();\n",
    "        done += 1;\n",
    "    print(\"Epoch: \", epoch, \"avg loss:\", running_sum/done, \"Time taken:\", time.time() - epoch_start, end = \"               \\r\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
